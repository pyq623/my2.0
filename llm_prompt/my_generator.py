# models/llm_config_generator.py
import json
import re
from typing import Dict, Any, Optional
from utils.llm_utils import call_llm_with_messages
from data import get_dataset_info

class LLMConfigGenerator:
    """LLM é…ç½®ç”Ÿæˆå™¨ - è´Ÿè´£ä¸ LLM äº¤äº’ç”Ÿæˆæ¨¡å‹é…ç½®"""
    
    def __init__(self, search_space: Dict[str, Any], constraint: Dict[str, Any], dataset_name: str):
        self.search_space = search_space
        self.constraint = constraint
        # self.llm = initialize_llm()
        self.global_insights = []  # å­˜å‚¨è·¨åˆ†æ”¯çš„æˆåŠŸç»éªŒ
        self.dataset_info = get_dataset_info(dataset_name)
        
    def generate_config_with_context(self, parent_config: Dict[str, Any], 
                                   direction: str,
                                   parent_performance: Dict[str, Any],
                                   global_insights: Dict = None,  # æ–°å¢å‚æ•°
                                   memory_feedback: str = None) -> Dict[str, Any]:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆæ–°é…ç½®"""
        
        # æ„å»ºæç¤ºè¯
        system_prompt = self._build_system_prompt(direction, global_insights)

        try:
            # ä½¿ç”¨å½“å‰çš„å†…å­˜åé¦ˆæ„å»ºæç¤ºè¯
            human_prompt = self._build_human_prompt(
                parent_config, direction, parent_performance, memory_feedback
            )
            
            print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)}")
            print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - ç”¨æˆ·æç¤ºè¯é•¿åº¦: {len(human_prompt)}")
            # ä½¿ç”¨ä¾¿æ·å‡½æ•°è°ƒç”¨ LLM
            response = call_llm_with_messages(system_prompt, human_prompt)
            
            print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ - LLM å“åº”: {response[:200]}...")  # åªæ‰“å°å‰ 200 ä¸ªå­—ç¬¦

            # è§£æå“åº”å¹¶æå– JSON é…ç½®
            new_config = self._parse_llm_response(response)
            
            # éªŒè¯é…ç½®çš„æœ‰æ•ˆæ€§å¹¶è‡ªåŠ¨ä¿®å¤
            validated_config = self._validate_and_fix_config(new_config, direction)
            if validated_config:
                # è®°å½•æˆåŠŸçš„é…ç½®ç»éªŒ
                # self._update_global_insights(direction, new_config)
                return validated_config
            else:
                print("LLM ç”Ÿæˆçš„é…ç½®æ— æ•ˆï¼Œä½¿ç”¨æ™ºèƒ½å˜å¼‚")
                return self._generate_base_config(direction)
                
        except Exception as e:
            print(f"LLM é…ç½®ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨æ™ºèƒ½å˜å¼‚")
            return self._generate_base_config(direction)
    
    def _build_system_prompt(self, direction: str, global_insights: Dict = None) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        # æ„å»ºç»éªŒéƒ¨åˆ†
        insights_text = ""

        if global_insights:
            useful_insights = []
            
            # æå–ä¸å½“å‰æ–¹å‘ç›¸å…³çš„ç»éªŒ
            for insight_key, insight_data in global_insights.items():
                if insight_key.startswith("direction_"):
                    dir_name = insight_key.replace("direction_", "")
                    success_rate = insight_data.get('success_rate', 0)
                    avg_reward = insight_data.get('average_reward', 0)
                    
                    # åªæ˜¾ç¤ºæˆåŠŸç‡è¾ƒé«˜çš„ç»éªŒ
                    if success_rate > 0.6:
                        useful_insights.append(
                            f"æ–¹å‘ '{dir_name}': æˆåŠŸç‡ {success_rate:.1%}, å¹³å‡å¥–åŠ± {avg_reward:.3f}"
                        )
            if useful_insights:
                insights_text = f"""
                GLOBAL EXPERIENCE INSIGHTS (from successful search branches):
                {chr(10).join(f"â€¢ {insight}" for insight in useful_insights)}
                """

        system_prompt = f"""
        You are a neural network architecture design expert. Your task is to generate improved network configurations based on given constraints and insights.
        {insights_text}

        **Conv Type in the Search Space:**
            1. DWSepConvBlock: Depthwise separable convolution (Depthwise + Pointwise) structure with skip connection support.
            2. MBConvBlock: Inverted residual structure (expansion convolution + Depthwise + SE module + Pointwise) with skip connection support.
            3. DpConvBlock: Pure depthwise convolution (Depthwise + Pointwise) structure without SE module or skip connections.
            4. SeSepConvBlock: Depthwise separable convolution with SE module (Depthwise + SE + Pointwise) structure.
            5. SeDpConvBlock: Depthwise convolution with SE module (Depthwise + SE) structure without Pointwise convolution.
        
        **Quantization Modes (IMPORTANT):**
            - none: No quantization - standard FP32 model (baseline)
            - static: Post-training static quantization - applies INT8 quantization after training (fast but may lose accuracy)
            - qat: Quantization-Aware Training - simulates quantization effects during training to improve accuracy after quantization
            - qaft: Quantization-Aware Fine-Tuning - fine-tunes a pre-trained model with quantization awareness to recover accuracy lost during quantization
            * RECOMMENDED for best accuracy-efficiency trade-off
            * Quantization may yield dramatic degration or slight degration in accuracy depending on model architecture.

        **Important Notes:**
            - In the search space, "DWSepConv" and "MBConv" both refer to "DWSepConv1D" and "MBConv1D", but when you generate the configuration, you should only write "DWSepConv" and "MBConv" according to the instructions in the search space.
            - If has_se is set to False, then se_ratios will be considered as 0, and vice versa. Conversely, if Has_se is set to True, then se_ratios must be greater than 0, and the same holds true in reverse.
            - In the search space, "DWSepConv" and "MBConv" both refer to "DWSepConv1D" and "MBConv1D", but when you generate the configuration, you should only write "DWSepConv" and "MBConv" according to the instructions in the search space.
            - "MBConv" is only different from "DWSeqConv" when expansion > 1, otherwise they are the same block.
            - If the type of a convolution block is "SeDpConv", then the `in_channels` and `out_channels` of this convolution block must be equal. This means that: - The `out_channels` of the previous convolution block must be equal to both the `in_channels` and `out_channels` of "SeDpConv".
            - If "SeDpConv" is a block in the first stage, its `channels` should be equal to `input_channels`, otherwise an error will be reported.
            
        Please ensure the configurations adhere to the defined search space constraints.
        """
        return system_prompt
    
    def _build_human_prompt(self, parent_config: Dict[str, Any], 
                          direction: str,
                          parent_performance: Dict[str, Any],
                          memory_feedback: str = None) -> str:
        """æ„å»ºç”¨æˆ·æç¤ºè¯"""
        channels = self.dataset_info['channels']
        time_steps = self.dataset_info['time_steps']
        num_classes = self.dataset_info['num_classes']
        parent_config = json.dumps(parent_config, indent=2) if parent_config else "No parent configuration available"
        
        max_peak_memory = self.constraint["max_peak_memory"]
        avg_reward = parent_performance.get('average_reward', 0)
        avg_reward = f"{avg_reward:.4f}"  # é™åˆ¶ä¸ºå°æ•°ç‚¹å4ä½
        visit_count = parent_performance.get('visit_count', 0)
        directions_explored = parent_performance.get('directions_explored', [])

        # æ·»åŠ å†…å­˜åé¦ˆåˆ°æç¤ºè¯
        memory_feedback_section = ""
        if memory_feedback:
            memory_feedback_section = f"""
            MEMORY FEEDBACK (Important!):
            {memory_feedback}
            
            Please generate a configuration that strictly adheres to the memory constraint.
            """

        human_prompt = """
        Parent Network Configuration:
        {parent_config}

        Parent Network Performance:
        - Average Reward: {avg_reward}
        - Visit Count: {visit_count}
        - Directions Explored: {directions_explored}

        Optimization Direction: {direction}
        (e.g., 'none' for origianl model without quantization, 'static' for static quantization, 'qat' for quantization-aware training, 'qaft' for Quantization-Aware Fine-Tuning.)

        {memory_feedback}
        
        TASK: Generate a new improved network configuration that:
        1. Uses the specified direction: {direction}
        2. Learns from the global experience insights above  
        3. Explores novel but promising architectural changes
        4. Maintains compatibility with the search space constraints, which is a must!
        5. Optimization direction is the specified quantization mode for this generation. The generated configured quantization mode must be the optimization direction.
        
        CONSTRAINTS (Search Space):
        Max memory: {max_peak_memory} MB

        SEARCH SPACE:
        {search_space}

        Please generate a new improved network configuration in valid JSON format. 
        The config you generated should have the same input channels and num classes in the example.
        For example:
        ```json
        {{
                "input_channels": {channels},  
                "num_classes": {classes},
                "quant_mode": "none",
                "stages": [
                    {{
                        "blocks": [
                            {{
                                "type": "DWSepConv",
                                "kernel_size": 3,
                                "expansion": 3,
                                "has_se": false,
                                "se_ratios": 0,
                                "skip_connection": false,
                                "stride": 1,
                                "activation": "ReLU6"
                            }}
                        ],
                        "channels": 8
                    }},
                    {{
                        "blocks": [
                            {{
                                "type": "MBConv",
                                "kernel_size": 3,
                                "expansion": 4,
                                "has_se": true,
                                "se_ratios": 0.25,
                                "skip_connection": true,
                                "stride": 2,
                                "activation": "Swish"
                            }}
                        ],
                        "channels": 16
                    }}
                ]
            }}
        ```
        
        Return ONLY the JSON configuration.""".format(
            direction = direction,
            parent_config = parent_config,
            channels=channels,
            classes=num_classes,
            avg_reward = avg_reward,
            visit_count = visit_count,
            directions_explored = directions_explored,
            max_peak_memory=max_peak_memory,
            search_space=json.dumps(self.search_space, indent=2),
            memory_feedback=memory_feedback_section  
        )
        return human_prompt
    
    def _parse_llm_response(self, response_text: str) -> Dict[str, Any]:
        """è§£æ LLM å“åº”ï¼Œæå– JSON é…ç½®"""
        try:
            # å°è¯•ç›´æ¥è§£æ JSON
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                return json.loads(json_str)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ° JSONï¼Œå°è¯•è§£æä¸ºä»£ç å—
                code_block_match = re.search(r'```(?:json)?\s*(.*?)\s*```', response_text, re.DOTALL)
                if code_block_match:
                    json_str = code_block_match.group(1)
                    return json.loads(json_str)
                else:
                    raise ValueError("No valid JSON found in response")
                    
        except (json.JSONDecodeError, ValueError) as e:
            print(f"JSON è§£æå¤±è´¥: {e}")
            raise
    
    def _validate_and_fix_config(self, config: Dict[str, Any], target_direction: str) -> Dict[str, Any]:
        """éªŒè¯é…ç½®çš„æœ‰æ•ˆæ€§"""
        required_fields = ['input_channels', 'num_classes', 'quant_mode', 'stages']
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in required_fields:
            if field not in config:
                print(f"Missing required field: {field}")
                return None
        
        # æ£€æŸ¥é˜¶æ®µç»“æ„
        if not isinstance(config['stages'], list):
            print("Stages must be a list")
            return None
        
        # å¼ºåˆ¶ä¿®å¤é‡åŒ–æ¨¡å¼
        if config.get('quant_mode') != target_direction:
            print(f"Auto-fixing: quant_mode should be '{target_direction}', but got '{config.get('quant_mode')}'. Setting to '{target_direction}'")
            config['quant_mode'] = target_direction
        
        # ä¿®å¤å’ŒéªŒè¯æ¯ä¸ªé˜¶æ®µ
        input_channels = config['input_channels']
        current_channels = input_channels
        
        for stage_idx, stage in enumerate(config['stages']):
            if 'channels' not in stage or 'blocks' not in stage:
                print("Invalid stage structure")
                return None
            
            if not isinstance(stage['blocks'], list) or len(stage['blocks']) == 0:
                print("Blocks must be a non-empty list")
                return None
            
            stage_channels = stage['channels']
            
            # éªŒè¯å’Œä¿®å¤æ¯ä¸ªblock
            for block_idx, block in enumerate(stage['blocks']):
                if not self._validate_and_fix_block(block, stage_idx, block_idx, current_channels, stage_channels, input_channels):
                    return None
                
                # å¤„ç† SeDpConv çš„é€šé“ä¿®å¤
                if block['type'] == 'SeDpConv' and '_stage_channels_fixed' in block:
                    # åº”ç”¨é€šé“ä¿®å¤
                    fixed_channels = block['_stage_channels_fixed']
                    stage['channels'] = fixed_channels
                    print(f"Applied SeDpConv channel fix: stage {stage_idx} channels set to {fixed_channels}")
                    # ç§»é™¤ä¸´æ—¶æ ‡è®°
                    del block['_stage_channels_fixed']
                    # æ›´æ–°å½“å‰é€šé“æ•°
                    current_channels = fixed_channels
                else:
                    # æ›´æ–°å½“å‰é€šé“æ•°ç”¨äºä¸‹ä¸€ä¸ªblock
                    if block['type'] == 'SeDpConv':
                        # SeDpConv ä¿æŒé€šé“æ•°ä¸å˜
                        current_channels = current_channels
                    else:
                        current_channels = stage['channels']
        
        return config
    
    def _validate_and_fix_block(self, block: Dict[str, Any], stage_idx: int, block_idx: int, 
                              current_channels: int, stage_channels: int, input_channels: int) -> bool:
        """éªŒè¯å’Œä¿®å¤å•ä¸ª block çš„é…ç½®"""
        required_block_fields = ['type', 'kernel_size', 'stride', 'expansion', 'has_se', 'se_ratio', 'skip_connection', 'activation']
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in required_block_fields:
            if field not in block:
                print(f"Block missing required field: {field}")
                return False
        
        # éªŒè¯å·ç§¯ç±»å‹
        valid_conv_types = ["DWSepConv", "MBConv", "DpConv", "SeSepConv", "SeDpConv"]
        if block['type'] not in valid_conv_types:
            print(f"Invalid conv type: {block['type']}")
            return False
        
        # å¤„ç† SeDpConv çš„ç‰¹æ®Šçº¦æŸ
        if block['type'] == 'SeDpConv':
            # SeDpConv è¦æ±‚è¾“å…¥è¾“å‡ºé€šé“ç›¸ç­‰
            if stage_idx == 0 and block_idx == 0:
                # ç¬¬ä¸€ä¸ªstageçš„ç¬¬ä¸€ä¸ªblockï¼Œé€šé“æ•°å¿…é¡»ç­‰äºinput_channels
                if stage_channels != input_channels:
                    print(f"Auto-fixing: SeDpConv in first block requires channels equal to input_channels ({input_channels}), changing from {stage_channels} to {input_channels}")
                    # ç›´æ¥ä¿®æ”¹stageçš„channels
                    block['_stage_channels_fixed'] = input_channels  # æ ‡è®°éœ€è¦ä¿®å¤
            else:
                # å…¶ä»–ä½ç½®çš„SeDpConvï¼Œé€šé“æ•°å¿…é¡»ç­‰äºå½“å‰è¾“å…¥é€šé“
                if stage_channels != current_channels:
                    print(f"Auto-fixing: SeDpConv requires channels equal to input channels ({current_channels}), changing from {stage_channels} to {current_channels}")
                    # ç›´æ¥ä¿®æ”¹stageçš„channels
                    block['_stage_channels_fixed'] = current_channels  # æ ‡è®°éœ€è¦ä¿®å¤
        
        # éªŒè¯å’Œä¿®å¤ SE æ¨¡å—è®¾ç½®
        if block['type'] in ['SeDpConv', 'SeSepConv']:
            # è¿™äº›ç±»å‹å¿…é¡»å¯ç”¨ SE
            if not block['has_se']:
                print(f"Auto-fixing: {block['type']} must have has_se=True")
                block['has_se'] = True
            
            if block['se_ratio'] <= 0:
                # è®¾ç½®åˆç†çš„é»˜è®¤ SE ratio
                block['se_ratio'] = 0.25
                print(f"Auto-fixing: {block['type']} must have se_ratio > 0, set to 0.25")
        
        # éªŒè¯ has_se å’Œ se_ratio çš„ä¸€è‡´æ€§
        if block['has_se'] and block['se_ratio'] <= 0:
            print(f"Auto-fixing: has_se=True but se_ratio={block['se_ratio']}, setting se_ratio=0.25")
            block['se_ratio'] = 0.25
        elif not block['has_se'] and block['se_ratio'] > 0:
            print(f"Auto-fixing: has_se=False but se_ratio={block['se_ratio']}, setting se_ratio=0")
            block['se_ratio'] = 0
        
        # éªŒè¯å’Œä¿®å¤ skip connection
        if block['type'] in ['DpConv', 'SeDpConv', 'SeSepConv']:
            # è¿™äº›ç±»å‹ä¸æ”¯æŒ skip connection
            if block['skip_connection']:
                print(f"Auto-fixing: {block['type']} does not support skip_connection, setting to False")
                block['skip_connection'] = False
        
        # éªŒè¯ MBConv å’Œ DWSepConv çš„å…³ç³»
        if block['type'] == 'MBConv' and block['expansion'] == 1:
            # MBConv with expansion=1 å®é™…ä¸Šå°±æ˜¯ DWSepConv
            print(f"Auto-fixing: MBConv with expansion=1 is equivalent to DWSepConv")
            block['expansion'] = 2
        elif block['type'] == 'DWSepConv' and block['expansion'] > 1:
            # DWSepConv with expansion>1 å®é™…ä¸Šå°±æ˜¯ MBConv
            print(f"Auto-fixing: DWSepConv with expansion>1 is equivalent to MBConv")
            block['expansion'] = 1
        
        # éªŒè¯ expansion èŒƒå›´
        if block['expansion'] < 1:
            print(f"Auto-fixing: expansion cannot be < 1, setting to 1")
            block['expansion'] = 1
        
        # éªŒè¯ kernel_size
        if block['kernel_size'] not in [1, 3, 5, 7]:
            print(f"Auto-fixing: invalid kernel_size {block['kernel_size']}, setting to 3")
            block['kernel_size'] = 3
        
        # éªŒè¯ stride (ç¬¬ä¸€ä¸ªblockå¯ä»¥æœ‰stride>1ï¼Œå…¶ä»–blockåº”è¯¥ä¸º1)
        if block_idx > 0 and block['stride'] > 1:
            print(f"Auto-fixing: only first block in stage can have stride>1, setting stride to 1")
            block['stride'] = 1
        
        # éªŒè¯ activation
        valid_activations = ['ReLU', 'ReLU6', 'Swish', 'HSwish', 'LeakyReLU']
        if block['activation'] not in valid_activations:
            print(f"Auto-fixing: invalid activation {block['activation']}, setting to ReLU")
            block['activation'] = 'ReLU'
        
        return True
    
    def _generate_base_config(self, direction: str) -> Dict[str, Any]:
        """ç”ŸæˆåŸºç¡€é…ç½®ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        print(f"ç”ŸæˆåŸºç¡€é…ç½®ï¼Œæ–¹å‘: {direction}")
        
        base_config = {
            "input_channels": self.dataset_info['channels'],
            "num_classes": self.dataset_info['num_classes'],
            "quant_mode": direction,
            "stages": [
                {
                    "blocks": [
                        {
                            "type": "DWSepConv",
                            "kernel_size": 3,
                            "stride": 1,
                            "expansion": 1,
                            "has_se": False,
                            "se_ratio": 0,
                            "skip_connection": False,
                            "activation": "ReLU6"
                        }
                    ],
                    "channels": 8
                },
                {
                    "blocks": [
                        {
                            "type": "MBConv",
                            "kernel_size": 3,
                            "stride": 2,
                            "expansion": 2,
                            "has_se": True,
                            "se_ratio": 0.25,
                            "skip_connection": True,
                            "activation": "Swish"
                        }
                    ],
                    "channels": 16
                }
            ]
        }
        
        return base_config
    
