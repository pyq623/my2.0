import torch
import torch.quantization as quantization
from torch.quantization import QuantStub, DeQuantStub
from .conv_blocks import MBConvBlock, DWSepConvBlock, SeDpConvBlock, DpConvBlock, SeSepConvBlock
from data import create_calibration_loader

def get_static_quantization_config(precision='int8'):
    """è·å–ä¸åŒç²¾åº¦çš„é™æ€é‡åŒ–é…ç½®ï¼ˆFBGEMMå…¼å®¹ç‰ˆï¼‰"""
    
    configs = {
        # ===== FBGEMMå…¼å®¹çš„INT8é…ç½® =====
        'int8': {
            'qconfig': quantization.get_default_qconfig('fbgemm'),
            'description': 'INT8 é»˜è®¤é‡åŒ–'
        },
        
        'int8_per_channel': {
            'qconfig': quantization.QConfig(
                activation=quantization.MinMaxObserver.with_args(
                    dtype=torch.quint8,  # ğŸ”‘ ä¿®æ­£ï¼šFBGEMMéœ€è¦æ— ç¬¦å·INT8æ¿€æ´»
                    qscheme=torch.per_tensor_affine  # ğŸ”‘ ä¿®æ­£ï¼šæ— ç¬¦å·INT8ä½¿ç”¨affine
                ),
                weight=quantization.PerChannelMinMaxObserver.with_args(
                    dtype=torch.qint8,   # æƒé‡ä½¿ç”¨æœ‰ç¬¦å·INT8
                    qscheme=torch.per_channel_symmetric
                )
            ),
            'description': 'INT8 é€é€šé“é‡åŒ–'
        },
        
        'int8_reduce_range': {
            'qconfig': quantization.QConfig(
                activation=quantization.MinMaxObserver.with_args(
                    dtype=torch.quint8,  # ğŸ”‘ ä¿®æ­£ï¼šFBGEMMéœ€è¦æ— ç¬¦å·INT8æ¿€æ´»
                    qscheme=torch.per_tensor_affine,
                    reduce_range=True
                ),
                weight=quantization.PerChannelMinMaxObserver.with_args(
                    dtype=torch.qint8,
                    qscheme=torch.per_channel_symmetric,
                    reduce_range=True
                )
            ),
            'description': 'INT8 å‡å°‘èŒƒå›´é‡åŒ– (æ›´ä¿å®ˆ)'
        },
        
        'int8_symmetric': {
            'qconfig': quantization.QConfig(
                activation=quantization.MinMaxObserver.with_args(
                    dtype=torch.qint8,   # æœ‰ç¬¦å·INT8æ¿€æ´»
                    qscheme=torch.per_tensor_symmetric
                ),
                weight=quantization.PerChannelMinMaxObserver.with_args(
                    dtype=torch.qint8,
                    qscheme=torch.per_channel_symmetric
                )
            ),
            'description': 'INT8 å¯¹ç§°é‡åŒ– (æœ‰ç¬¦å·æ¿€æ´»)'
        },
        
        'int8_fbgemm_optimized': {
            'qconfig': quantization.QConfig(
                activation=quantization.MinMaxObserver.with_args(
                    dtype=torch.quint8,  # FBGEMMä¼˜åŒ–ï¼šæ— ç¬¦å·æ¿€æ´»
                    qscheme=torch.per_tensor_affine,
                    reduce_range=False
                ),
                weight=quantization.PerChannelMinMaxObserver.with_args(
                    dtype=torch.qint8,   # æœ‰ç¬¦å·æƒé‡
                    qscheme=torch.per_channel_symmetric
                )
            ),
            'description': 'FBGEMMä¼˜åŒ–é…ç½®'
        },
        
        # ===== QNNPACKé…ç½®ï¼ˆç§»åŠ¨ç«¯ï¼‰ =====
        'qnnpack': {
            'qconfig': quantization.get_default_qconfig('qnnpack'),
            'description': 'QNNPACK INT8é‡åŒ– (ç§»åŠ¨ç«¯ä¼˜åŒ–)'
        },
        
        'int8_qnnpack_custom': {
            'qconfig': quantization.QConfig(
                activation=quantization.MinMaxObserver.with_args(
                    dtype=torch.quint8,  # QNNPACKä¹Ÿä½¿ç”¨æ— ç¬¦å·æ¿€æ´»
                    qscheme=torch.per_tensor_affine
                ),
                weight=quantization.MinMaxObserver.with_args(  # QNNPACKä½¿ç”¨per-tensoræƒé‡
                    dtype=torch.qint8,
                    qscheme=torch.per_tensor_symmetric
                )
            ),
            'description': 'QNNPACKè‡ªå®šä¹‰é…ç½®'
        },
        
        # ===== é«˜å…¼å®¹æ€§é…ç½® =====
        'int8_simple': {
            'qconfig': quantization.QConfig(
                activation=quantization.MinMaxObserver.with_args(
                    dtype=torch.quint8,  # ğŸ”‘ ç»Ÿä¸€ä½¿ç”¨æ— ç¬¦å·æ¿€æ´»
                    qscheme=torch.per_tensor_affine
                ),
                weight=quantization.MinMaxObserver.with_args(
                    dtype=torch.qint8,
                    qscheme=torch.per_tensor_symmetric
                )
            ),
            'description': 'INT8 ç®€åŒ–é…ç½® (æœ€é«˜å…¼å®¹æ€§)'
        },
        
        # ===== ç›´æ–¹å›¾å’Œç§»åŠ¨å¹³å‡è§‚å¯Ÿå™¨ =====
        'histogram': {
            'qconfig': quantization.QConfig(
                activation=quantization.HistogramObserver.with_args(
                    dtype=torch.quint8,  # ğŸ”‘ ä¿®æ­£
                    qscheme=torch.per_tensor_affine
                ),
                weight=quantization.PerChannelMinMaxObserver.with_args(
                    dtype=torch.qint8,
                    qscheme=torch.per_channel_symmetric
                )
            ),
            'description': 'INT8 ç›´æ–¹å›¾è§‚å¯Ÿå™¨'
        },

        'moving_average': {
            'qconfig': quantization.QConfig(
                activation=quantization.MovingAverageMinMaxObserver.with_args(
                    dtype=torch.quint8,  # ğŸ”‘ ä¿®æ­£
                    qscheme=torch.per_tensor_affine
                ),
                weight=quantization.PerChannelMinMaxObserver.with_args(
                    dtype=torch.qint8,
                    qscheme=torch.per_channel_symmetric
                )
            ),
            'description': 'INT8 ç§»åŠ¨å¹³å‡è§‚å¯Ÿå™¨'
        },
    }
    
    if precision not in configs:
        print(f"âš ï¸ æœªçŸ¥çš„é‡åŒ–ç²¾åº¦: {precision}, ä½¿ç”¨é»˜è®¤çš„ int8")
        precision = 'int8'
    
    return configs[precision]

# æ›´æ–°çš„é™æ€é‡åŒ–é…ç½®é€‰é¡¹
STATIC_QUANTIZATION_OPTIONS = {
    # INT8 ç³»åˆ—
    'int8_default': {
        'precision': 'int8',
        'backend': 'fbgemm',
        'description': 'é»˜è®¤INT8é‡åŒ–',
        'memory_saving': '~75%',
        'precision_loss': 'ä¸­ç­‰'
    },
    'int8_per_channel': {
        'precision': 'int8_per_channel',
        'backend': 'fbgemm',
        'description': 'INT8é€é€šé“é‡åŒ– (æ›´é«˜ç²¾åº¦)',
        'memory_saving': '~75%',
        'precision_loss': 'è¾ƒå°'
    },
    'int8_reduce_range': {
        'precision': 'int8_reduce_range',
        'backend': 'fbgemm',
        'description': 'INT8å‡å°‘èŒƒå›´ (æ›´ä¿å®ˆ)',
        'memory_saving': '~75%',
        'precision_loss': 'å¾ˆå°'
    },
    'int8_asymmetric': {
        'precision': 'int8_asymmetric',
        'backend': 'fbgemm',
        'description': 'INT8éå¯¹ç§°é‡åŒ–',
        'memory_saving': '~75%',
        'precision_loss': 'ä¸­ç­‰'
    },
    'int8_mobile': {
        'precision': 'qnnpack',
        'backend': 'qnnpack',
        'description': 'QNNPACKç§»åŠ¨ç«¯ä¼˜åŒ–',
        'memory_saving': '~75%',
        'precision_loss': 'ä¸­ç­‰'
    },
    'int8_histogram': {
        'precision': 'histogram',
        'backend': 'fbgemm',
        'description': 'INT8ç›´æ–¹å›¾æ ¡å‡†',
        'memory_saving': '~75%',
        'precision_loss': 'è¾ƒå°'
    },
    'int8_moving_avg': {
        'precision': 'moving_average',
        'backend': 'fbgemm',
        'description': 'INT8ç§»åŠ¨å¹³å‡æ ¡å‡†',
        'memory_saving': '~75%',
        'precision_loss': 'è¾ƒå°'
    }
}

def get_quantization_option(option_name):
    """è·å–é¢„å®šä¹‰çš„é‡åŒ–é€‰é¡¹"""
    return STATIC_QUANTIZATION_OPTIONS.get(option_name, STATIC_QUANTIZATION_OPTIONS['int8_default'])


def print_available_quantization_options():
    """æ‰“å°æ‰€æœ‰å¯ç”¨çš„é‡åŒ–é€‰é¡¹"""
    print("\n=== å¯ç”¨çš„é‡åŒ–é…ç½®é€‰é¡¹ ===")
    print(f"{'é€‰é¡¹åç§°':<20} {'æè¿°':<30} {'å†…å­˜èŠ‚çœ':<10} {'ç²¾åº¦æŸå¤±':<10}")
    print("-" * 70)
    
    for name, config in STATIC_QUANTIZATION_OPTIONS.items():
        print(f"{name:<20} {config['description']:<30} {config['memory_saving']:<10} {config['precision_loss']:<10}")
    print()

def fuse_model_modules(model):
    print("âš™ï¸ å¼€å§‹ç®—å­èåˆ...")
    model.eval()
    for module in model.modules():
        if isinstance(module, MBConvBlock):
            if hasattr(module, 'expand_conv'):
                torch.quantization.fuse_modules(module.expand_conv, ['0', '1'], inplace=True)
            if hasattr(module, 'dw_conv'):
                torch.quantization.fuse_modules(module.dw_conv, ['0', '1'], inplace=True)
            if hasattr(module, 'pw_conv'):
                torch.quantization.fuse_modules(module.pw_conv, ['0', '1'], inplace=True)
        elif isinstance(module, DWSepConvBlock):
            if hasattr(module, 'dw_conv'):
                torch.quantization.fuse_modules(module.dw_conv, ['0', '1'], inplace=True)
            if hasattr(module, 'pw_conv'):
                torch.quantization.fuse_modules(module.pw_conv, ['0', '1'], inplace=True)
        # å¯¹ SeDpConvBlock è¿›è¡Œèåˆ
        elif isinstance(module, SeDpConvBlock):
            if hasattr(module, 'dw_conv'):
                torch.quantization.fuse_modules(module.dw_conv, ['0', '1'], inplace=True)
        # å¯¹ DpConvBlock è¿›è¡Œèåˆ
        elif isinstance(module, DpConvBlock):
            if hasattr(module, 'dw_conv'):
                torch.quantization.fuse_modules(module.dw_conv, ['0', '1'], inplace=True)
            if hasattr(module, 'pw_conv'):
                torch.quantization.fuse_modules(module.pw_conv, ['0', '1'], inplace=True) 
        # å¯¹ SeSepConvBlock è¿›è¡Œèåˆ
        elif isinstance(module, SeSepConvBlock):
            if hasattr(module, 'dw_conv'):
                torch.quantization.fuse_modules(module.dw_conv, ['0', '1'], inplace=True)
            if hasattr(module, 'pw_conv'):
                torch.quantization.fuse_modules(module.pw_conv, ['0', '1'], inplace=True)
    print("âœ… ç®—å­èåˆå®Œæˆã€‚")

def fuse_QATmodel_modules(model):
    print("âš™ï¸ å¼€å§‹ç®—å­èåˆ...")
    # å¿…é¡»åœ¨è®­ç»ƒæ¨¡å¼ä¸‹è¿›è¡ŒQATå‡†å¤‡
    model.eval()
    # è·å–æ‰€æœ‰éœ€è¦èåˆçš„æ¨¡å—
    modules_to_fuse = []
    for name, module in model.named_modules():
        if isinstance(module, MBConvBlock):
            # MBConvBlockçš„èåˆ
            if hasattr(module, 'expand_conv') and module.expand_conv is not None:
                expand_conv_0 = f"{name}.expand_conv.0"
                expand_conv_1 = f"{name}.expand_conv.1"
                if expand_conv_0 in dict(model.named_modules()) and expand_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([expand_conv_0, expand_conv_1])
            if hasattr(module, 'dw_conv') and module.dw_conv is not None:
                dw_conv_0 = f"{name}.dw_conv.0"
                dw_conv_1 = f"{name}.dw_conv.1"
                if dw_conv_0 in dict(model.named_modules()) and dw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([dw_conv_0, dw_conv_1])
            if hasattr(module, 'pw_conv') and module.pw_conv is not None:
                pw_conv_0 = f"{name}.pw_conv.0"
                pw_conv_1 = f"{name}.pw_conv.1"
                if pw_conv_0 in dict(model.named_modules()) and pw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([pw_conv_0, pw_conv_1])
                
        elif isinstance(module, DWSepConvBlock):
            # DWSepConvBlockçš„èåˆ
            if hasattr(module, 'dw_conv') and module.dw_conv is not None:
                dw_conv_0 = f"{name}.dw_conv.0"
                dw_conv_1 = f"{name}.dw_conv.1"
                if dw_conv_0 in dict(model.named_modules()) and dw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([dw_conv_0, dw_conv_1])
            if hasattr(module, 'pw_conv') and module.pw_conv is not None:
                pw_conv_0 = f"{name}.pw_conv.0"
                pw_conv_1 = f"{name}.pw_conv.1"
                if pw_conv_0 in dict(model.named_modules()) and pw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([pw_conv_0, pw_conv_1])
        # SeDpConvBlockçš„èåˆ
        elif isinstance(module, SeDpConvBlock):
            if hasattr(module, 'dw_conv') and module.dw_conv is not None:
                dw_conv_0 = f"{name}.dw_conv.0"
                dw_conv_1 = f"{name}.dw_conv.1"
                if dw_conv_0 in dict(model.named_modules()) and dw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([dw_conv_0, dw_conv_1])
        
        # DpConvBlockçš„èåˆ
        elif isinstance(module, DpConvBlock):
            if hasattr(module, 'dw_conv') and module.dw_conv is not None:
                dw_conv_0 = f"{name}.dw_conv.0"
                dw_conv_1 = f"{name}.dw_conv.1"
                if dw_conv_0 in dict(model.named_modules()) and dw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([dw_conv_0, dw_conv_1])
            if hasattr(module, 'pw_conv') and module.pw_conv is not None:
                pw_conv_0 = f"{name}.pw_conv.0"
                pw_conv_1 = f"{name}.pw_conv.1"
                if pw_conv_0 in dict(model.named_modules()) and pw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([pw_conv_0, pw_conv_1])
        
        # SeSepConvBlockçš„èåˆ
        elif isinstance(module, SeSepConvBlock):
            if hasattr(module, 'dw_conv') and module.dw_conv is not None:
                dw_conv_0 = f"{name}.dw_conv.0"
                dw_conv_1 = f"{name}.dw_conv.1"
                if dw_conv_0 in dict(model.named_modules()) and dw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([dw_conv_0, dw_conv_1])
            if hasattr(module, 'pw_conv') and module.pw_conv is not None:
                pw_conv_0 = f"{name}.pw_conv.0"
                pw_conv_1 = f"{name}.pw_conv.1"
                if pw_conv_0 in dict(model.named_modules()) and pw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([pw_conv_0, pw_conv_1])
    
    # æ‰§è¡Œèåˆ
    try:
        if modules_to_fuse:
            torch.quantization.fuse_modules(model, modules_to_fuse, inplace=True)
            print(f"âœ… èåˆäº† {len(modules_to_fuse)} ç»„æ¨¡å—")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°éœ€è¦èåˆçš„æ¨¡å—")
    except Exception as e:
        print(f"âŒ æ¨¡å—èåˆå¤±è´¥: {str(e)}")

def apply_configurable_static_quantization(trained_model, dataloader, precision='int8', backend='fbgemm'):
    """åº”ç”¨å¯é…ç½®çš„é™æ€é‡åŒ–"""
    print(f"ğŸ”§ å¼€å§‹é™æ€é‡åŒ– - ç²¾åº¦: {precision}, åç«¯: {backend}")
    
    # è®¾ç½®é‡åŒ–åç«¯
    torch.backends.quantized.engine = backend
    trained_model.to('cpu').eval()
    
    # èåˆæ¨¡å—
    print("âš™ï¸ èåˆæ¨¡å—...")
    fuse_model_modules(trained_model)
    
    # è·å–é‡åŒ–é…ç½®
    quant_config = get_static_quantization_config(precision)
    print(f"ğŸ“‹ ä½¿ç”¨é…ç½®: {quant_config['description']}")
    
    # åº”ç”¨é‡åŒ–é…ç½®
    trained_model.qconfig = quant_config['qconfig']
    
    # å‡†å¤‡é‡åŒ–
    print("âš™ï¸ å‡†å¤‡é‡åŒ–...")
    quantization.prepare(trained_model, inplace=True)
    
    # æ ¡å‡†é˜¶æ®µ
    print("âš™ï¸ å¼€å§‹æ ¡å‡†...")
    calibration_loader = create_calibration_loader(dataloader['train'], num_batches=12)
    
    trained_model.eval()
    with torch.no_grad():
        for batch_idx, (inputs, _) in enumerate(calibration_loader):
            inputs = inputs.to('cpu')
            if inputs.dtype != torch.float32:
                inputs = inputs.float()
            
            try:
                _ = trained_model(inputs)
            except Exception as e:
                print(f"âš ï¸ æ ¡å‡†æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                continue
                
            if (batch_idx + 1) % 4 == 0:
                print(f"  æ ¡å‡†è¿›åº¦: {batch_idx + 1}/12")
    
    print("âœ… æ ¡å‡†å®Œæˆ")
    
    # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
    print("âš™ï¸ è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹...")
    try:
        quantized_model = quantization.convert(trained_model, inplace=True)
        
        # åˆ†æé‡åŒ–ç»“æœ
        # analyze_quantization_result(quantized_model, precision)
        
        print(f"âœ… {precision} é™æ€é‡åŒ–å®Œæˆ")
        return quantized_model
        
    except Exception as e:
        print(f"âŒ {precision} é‡åŒ–å¤±è´¥: {e}")
        print("ğŸ”„ å›é€€åˆ°é»˜è®¤INT8é‡åŒ–...")
        
        # å›é€€åˆ°é»˜è®¤é…ç½®
        trained_model.qconfig = quantization.get_default_qconfig('fbgemm')
        quantization.prepare(trained_model, inplace=True)
        
        # é‡æ–°æ ¡å‡†
        with torch.no_grad():
            for inputs, _ in calibration_loader:
                inputs = inputs.to('cpu')
                if inputs.dtype != torch.float32:
                    inputs = inputs.float()
                _ = trained_model(inputs)
        
        quantized_model = quantization.convert(trained_model, inplace=True)
        print("âœ… é»˜è®¤é‡åŒ–å®Œæˆ")
        return quantized_model


def analyze_quantization_result(model, precision):
    """åˆ†æé‡åŒ–ç»“æœ"""
    print(f"\n === {precision} é‡åŒ–ç»“æœåˆ†æ === ")
    
    total_params = 0
    quantized_params = 0
    total_memory = 0
    quantized_memory = 0
    
    for name, param in model.named_parameters():
        param_count = param.numel()
        param_memory = param_count * param.element_size()
        
        total_params += param_count
        total_memory += param_memory
        
        if 'qint' in str(param.dtype):
            quantized_params += param_count
            quantized_memory += param_memory
            status = "âœ… å·²é‡åŒ–"
        else:
            status = "âŒ æœªé‡åŒ–"
        
        print(f"{name}:")
        print(f"  å‚æ•°æ•°: {param_count:,}")
        print(f"  ç±»å‹: {param.dtype}")
        print(f"  å†…å­˜: {param_memory/1024/1024:.3f} MB")
        print(f"  çŠ¶æ€: {status}")
        print()
    
    # ç»Ÿè®¡ç»“æœ
    quantization_ratio = quantized_params / total_params * 100 if total_params > 0 else 0
    memory_ratio = quantized_memory / total_memory * 100 if total_memory > 0 else 0
    
    print("=== é‡åŒ–ç»Ÿè®¡ ===")
    print(f"æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"é‡åŒ–å‚æ•°æ•°: {quantized_params:,}")
    print(f"é‡åŒ–æ¯”ä¾‹: {quantization_ratio:.1f}%")
    print(f"æ€»å†…å­˜: {total_memory/1024/1024:.2f} MB")
    print(f"é‡åŒ–åå†…å­˜: {quantized_memory/1024/1024:.2f} MB") 
    print(f"å†…å­˜é‡åŒ–æ¯”ä¾‹: {memory_ratio:.1f}%")
    
    # ä¼°ç®—å†…å­˜èŠ‚çœ
    if total_params > 0:
        fp32_memory = total_params * 4 / 1024 / 1024  # FP32åŸºå‡†
        current_memory = total_memory / 1024 / 1024
        memory_saving = (1 - current_memory / fp32_memory) * 100
        print(f"ç›¸æ¯”FP32å†…å­˜èŠ‚çœ: {memory_saving:.1f}%")
    
    return {
        'quantization_ratio': quantization_ratio,
        'memory_saving': memory_saving if total_params > 0 else 0
    }


class QuantizableModel(torch.nn.Module):
    """
    å¯é‡åŒ–æ¨¡å‹åŒ…è£…å™¨ï¼Œæ·»åŠ é‡åŒ–stub
    """
    def __init__(self, model):
        super(QuantizableModel, self).__init__()
        self.model = model
        self.quant = QuantStub()
        self.dequant = DeQuantStub()
        self.quant_mode = None  # æ·»åŠ é‡åŒ–æ¨¡å¼æ ‡å¿—
        self.quant_enabled = True
        
    def enable_quant(self):
        self.quant_enabled = True
        
    def disable_quant(self):
        self.quant_enabled = False

    def set_quant_mode(self, mode):
        self.quant_mode = mode
    
    def forward(self, x):
        if self.quant_mode in ['dynamic', 'static', 'qat']:  # ä»…åœ¨é‡åŒ–æ¨¡å¼ä¸‹æ‰§è¡Œ
            x = self.quant(x)
            x = self.model(x)
            x = self.dequant(x)
        else:
            x = self.model(x)
        return x


    # æ·»åŠ ä»¥ä¸‹æ–¹æ³•ä»¥ä¿æŒä¸åŸå§‹æ¨¡å‹çš„å…¼å®¹æ€§
    @property
    def output_dim(self):
        return self.model.output_dim

    # æ·»åŠ çŠ¶æ€å­—å…¸å¤„ç†
    def load_state_dict(self, state_dict, strict=True):
        # å…ˆå°è¯•ç›´æ¥åŠ è½½
        try:
            super().load_state_dict(state_dict, strict)
        except RuntimeError as e:
            if "Missing key(s)" in str(e):
                # ä¿®å¤çŠ¶æ€å­—å…¸é”®å
                new_state_dict = {}
                for k, v in state_dict.items():
                    if k.startswith('model.'):
                        new_state_dict[k[6:]] = v
                    else:
                        new_state_dict[k] = v
                super().load_state_dict(new_state_dict, strict=False)
            else:
                raise e
            
    # æ·»åŠ é™æ€é‡åŒ–æ”¯æŒ
    def prepare_static_quantization(self):
        """
        å‡†å¤‡é™æ€é‡åŒ–ï¼šä¸ºæ¨¡å‹æ’å…¥è§‚å¯Ÿå™¨ã€‚
        """
        self.eval()  # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
        self.qconfig = torch.quantization.get_default_qconfig('fbgemm')  #  fbgemm æˆ– 'qnnpack'
        # ç‰¹åˆ«å¤„ç†GroupNormå±‚
        torch.quantization.quantize_default_mappings[torch.nn.GroupNorm] = torch.quantization.default_float_to_quantized_operator_mappings[torch.nn.GroupNorm]
        torch.quantization.prepare(self, inplace=True)  # æ’å…¥è§‚å¯Ÿå™¨
        print("é‡åŒ–å‡†å¤‡åçš„æ¨¡å‹ç»“æ„:")
        print(self)  # æ‰“å°æ¨¡å‹ç»“æ„ï¼Œæ£€æŸ¥è§‚å¯Ÿå™¨æ˜¯å¦æ­£ç¡®æ’å…¥

    def convert_static_quantization(self):
        """
        è½¬æ¢ä¸ºé™æ€é‡åŒ–æ¨¡å‹
        """
        quantization.convert(self, inplace=True)