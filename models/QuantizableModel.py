import torch
import torch.quantization as quantization
from torch.quantization import QuantStub, DeQuantStub
from .conv_blocks import MBConvBlock, DWSepConvBlock, SeDpConvBlock, DpConvBlock, SeSepConvBlock
from data import create_calibration_loader


def evaluate_quantized_model(quantized_model, dataloader, task_head, description="é‡åŒ–æ¨¡å‹"):
    print(f"\n=== å¼€å§‹è¯„ä¼° {description} ===", flush=True)
    quantized_model.eval()
    task_head.eval()

    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    import gc
    gc.collect()
    torch.cuda.empty_cache()

    correct = 0
    total = 0

    # æ·»åŠ æ›´å¤šè°ƒè¯•ç‚¹
    print("æ¨¡å‹å’Œè®¾å¤‡ä¿¡æ¯:", flush=True)
    print(f"é‡åŒ–æ¨¡å‹ç±»å‹: {type(quantized_model)}", flush=True)
    print(f"ä»»åŠ¡å¤´è®¾å¤‡: {next(task_head.parameters()).device}", flush=True)
    
    try:
        with torch.no_grad():
            # å…ˆæµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
            test_batch = next(iter(dataloader['test']))
            print("æˆåŠŸè·å–æµ‹è¯•æ‰¹æ¬¡", flush=True)
            
            for batch_idx, (inputs, labels) in enumerate(dataloader['test']):
                # print(f"\nå¤„ç†æ‰¹æ¬¡ {batch_idx}", flush=True)
                
                inputs = inputs.to('cpu')
                labels = labels.to('cpu')
                # print(f"è¾“å…¥å½¢çŠ¶: {inputs.shape}", flush=True)
                
                try:
                    # è·å–é‡åŒ–æ¨¡å‹çš„è¾“å‡ºç‰¹å¾
                    features = quantized_model(inputs)
                    # print(f"ç‰¹å¾ç±»å‹: {type(features)}", flush=True)
                    
                    if not isinstance(features, torch.Tensor):
                        # print("æ‰§è¡Œåé‡åŒ–...", flush=True)
                        features = features.dequantize()
                    
                    if features.device != torch.device('cpu'):
                        features = features.to('cpu')
                    
                    # # æ£€æŸ¥ç»´åº¦
                    # if features.shape[-1] != task_head.in_features:
                    #     raise ValueError(f"ç»´åº¦ä¸åŒ¹é…: {features.shape[-1]} != {task_head.in_features}")
                    
                    # åˆ†ç±»
                    outputs = task_head(features)
                    _, predicted = outputs.max(1)
                    
                    batch_total = labels.size(0)
                    batch_correct = predicted.eq(labels).sum().item()
                    total += batch_total
                    correct += batch_correct
                    
                    # print(f"æ‰¹æ¬¡ç»“æœ: total={batch_total} correct={batch_correct}", flush=True)
                    # print(f"ç´¯è®¡ç»“æœ: total={total} correct={correct}", flush=True)
                    
                    # æå‰é€€å‡ºæµ‹è¯•
                    # if batch_idx >= 4:  # åªæµ‹è¯•å‰å‡ ä¸ªæ‰¹æ¬¡
                    #     break
                except Exception as batch_e:
                    print(f"æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {str(batch_e)}", flush=True)
                    continue
    
                # æ‰‹åŠ¨æ¸…ç†æ‰¹æ¬¡æ•°æ®
                del inputs, labels, features, outputs, predicted
                gc.collect()

        print(f"æœ€ç»ˆç»Ÿè®¡: total={total} correct={correct}", flush=True)
        quant_accuracy = 100. * correct / total if total > 0 else 0
        print(f"{description} æµ‹è¯•å‡†ç¡®ç‡: {quant_accuracy:.2f}%", flush=True)
        return quant_accuracy
    
    except Exception as e:
        print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}", flush=True)
        return 0.0
    
    finally:
        # æ˜¾å¼æ¸…ç†
        torch.cuda.empty_cache()
        print("è¯„ä¼°å®Œæˆï¼Œèµ„æºå·²æ¸…ç†", flush=True)


def get_static_quantization_config(precision='int8'):
    """è·å–ä¸åŒç²¾åº¦çš„é™æ€é‡åŒ–é…ç½®ï¼ˆFBGEMMå…¼å®¹ç‰ˆï¼‰"""
    
    configs = {
        # ===== FBGEMMå…¼å®¹çš„INT8é…ç½® =====
        'int8': {
            'qconfig': quantization.get_default_qconfig('fbgemm'),
            'description': 'INT8 é»˜è®¤é‡åŒ–'
        },
        
        'int8_per_channel': {
            'qconfig': quantization.QConfig(
                activation=quantization.MinMaxObserver.with_args(
                    dtype=torch.quint8,  # ğŸ”‘ ä¿®æ­£ï¼šFBGEMMéœ€è¦æ— ç¬¦å·INT8æ¿€æ´»
                    qscheme=torch.per_tensor_affine  # ğŸ”‘ ä¿®æ­£ï¼šæ— ç¬¦å·INT8ä½¿ç”¨affine
                ),
                weight=quantization.PerChannelMinMaxObserver.with_args(
                    dtype=torch.qint8,   # æƒé‡ä½¿ç”¨æœ‰ç¬¦å·INT8
                    qscheme=torch.per_channel_symmetric
                )
            ),
            'description': 'INT8 é€é€šé“é‡åŒ–'
        },
        
        'int8_reduce_range': {
            'qconfig': quantization.QConfig(
                activation=quantization.MinMaxObserver.with_args(
                    dtype=torch.quint8,  # ğŸ”‘ ä¿®æ­£ï¼šFBGEMMéœ€è¦æ— ç¬¦å·INT8æ¿€æ´»
                    qscheme=torch.per_tensor_affine,
                    reduce_range=True
                ),
                weight=quantization.PerChannelMinMaxObserver.with_args(
                    dtype=torch.qint8,
                    qscheme=torch.per_channel_symmetric,
                    reduce_range=True
                )
            ),
            'description': 'INT8 å‡å°‘èŒƒå›´é‡åŒ– (æ›´ä¿å®ˆ)'
        },
        
        'int8_symmetric': {
            'qconfig': quantization.QConfig(
                activation=quantization.MinMaxObserver.with_args(
                    dtype=torch.qint8,   # æœ‰ç¬¦å·INT8æ¿€æ´»
                    qscheme=torch.per_tensor_symmetric
                ),
                weight=quantization.PerChannelMinMaxObserver.with_args(
                    dtype=torch.qint8,
                    qscheme=torch.per_channel_symmetric
                )
            ),
            'description': 'INT8 å¯¹ç§°é‡åŒ– (æœ‰ç¬¦å·æ¿€æ´»)'
        },
        
        'int8_fbgemm_optimized': {
            'qconfig': quantization.QConfig(
                activation=quantization.MinMaxObserver.with_args(
                    dtype=torch.quint8,  # FBGEMMä¼˜åŒ–ï¼šæ— ç¬¦å·æ¿€æ´»
                    qscheme=torch.per_tensor_affine,
                    reduce_range=False
                ),
                weight=quantization.PerChannelMinMaxObserver.with_args(
                    dtype=torch.qint8,   # æœ‰ç¬¦å·æƒé‡
                    qscheme=torch.per_channel_symmetric
                )
            ),
            'description': 'FBGEMMä¼˜åŒ–é…ç½®'
        },
        
        # ===== QNNPACKé…ç½®ï¼ˆç§»åŠ¨ç«¯ï¼‰ =====
        'qnnpack': {
            'qconfig': quantization.get_default_qconfig('qnnpack'),
            'description': 'QNNPACK INT8é‡åŒ– (ç§»åŠ¨ç«¯ä¼˜åŒ–)'
        },
        
        'int8_qnnpack_custom': {
            'qconfig': quantization.QConfig(
                activation=quantization.MinMaxObserver.with_args(
                    dtype=torch.quint8,  # QNNPACKä¹Ÿä½¿ç”¨æ— ç¬¦å·æ¿€æ´»
                    qscheme=torch.per_tensor_affine
                ),
                weight=quantization.MinMaxObserver.with_args(  # QNNPACKä½¿ç”¨per-tensoræƒé‡
                    dtype=torch.qint8,
                    qscheme=torch.per_tensor_symmetric
                )
            ),
            'description': 'QNNPACKè‡ªå®šä¹‰é…ç½®'
        },
        
        # ===== é«˜å…¼å®¹æ€§é…ç½® =====
        'int8_simple': {
            'qconfig': quantization.QConfig(
                activation=quantization.MinMaxObserver.with_args(
                    dtype=torch.quint8,  # ğŸ”‘ ç»Ÿä¸€ä½¿ç”¨æ— ç¬¦å·æ¿€æ´»
                    qscheme=torch.per_tensor_affine
                ),
                weight=quantization.MinMaxObserver.with_args(
                    dtype=torch.qint8,
                    qscheme=torch.per_tensor_symmetric
                )
            ),
            'description': 'INT8 ç®€åŒ–é…ç½® (æœ€é«˜å…¼å®¹æ€§)'
        },
        
        # ===== ç›´æ–¹å›¾å’Œç§»åŠ¨å¹³å‡è§‚å¯Ÿå™¨ =====
        'histogram': {
            'qconfig': quantization.QConfig(
                activation=quantization.HistogramObserver.with_args(
                    dtype=torch.quint8,  # ğŸ”‘ ä¿®æ­£
                    qscheme=torch.per_tensor_affine
                ),
                weight=quantization.PerChannelMinMaxObserver.with_args(
                    dtype=torch.qint8,
                    qscheme=torch.per_channel_symmetric
                )
            ),
            'description': 'INT8 ç›´æ–¹å›¾è§‚å¯Ÿå™¨'
        },

        'moving_average': {
            'qconfig': quantization.QConfig(
                activation=quantization.MovingAverageMinMaxObserver.with_args(
                    dtype=torch.quint8,  # ğŸ”‘ ä¿®æ­£
                    qscheme=torch.per_tensor_affine
                ),
                weight=quantization.PerChannelMinMaxObserver.with_args(
                    dtype=torch.qint8,
                    qscheme=torch.per_channel_symmetric
                )
            ),
            'description': 'INT8 ç§»åŠ¨å¹³å‡è§‚å¯Ÿå™¨'
        },
    }
    
    if precision not in configs:
        print(f"âš ï¸ æœªçŸ¥çš„é‡åŒ–ç²¾åº¦: {precision}, ä½¿ç”¨é»˜è®¤çš„ int8")
        precision = 'int8'
    
    return configs[precision]

# æ›´æ–°çš„é™æ€é‡åŒ–é…ç½®é€‰é¡¹
STATIC_QUANTIZATION_OPTIONS = {
    # INT8 ç³»åˆ—
    'int8_default': {
        'precision': 'int8',
        'backend': 'fbgemm',
        'description': 'é»˜è®¤INT8é‡åŒ–',
        'memory_saving': '~75%',
        'precision_loss': 'ä¸­ç­‰'
    },
    'int8_per_channel': {
        'precision': 'int8_per_channel',
        'backend': 'fbgemm',
        'description': 'INT8é€é€šé“é‡åŒ– (æ›´é«˜ç²¾åº¦)',
        'memory_saving': '~75%',
        'precision_loss': 'è¾ƒå°'
    },
    'int8_reduce_range': {
        'precision': 'int8_reduce_range',
        'backend': 'fbgemm',
        'description': 'INT8å‡å°‘èŒƒå›´ (æ›´ä¿å®ˆ)',
        'memory_saving': '~75%',
        'precision_loss': 'å¾ˆå°'
    },
    'int8_asymmetric': {
        'precision': 'int8_asymmetric',
        'backend': 'fbgemm',
        'description': 'INT8éå¯¹ç§°é‡åŒ–',
        'memory_saving': '~75%',
        'precision_loss': 'ä¸­ç­‰'
    },
    'int8_mobile': {
        'precision': 'qnnpack',
        'backend': 'qnnpack',
        'description': 'QNNPACKç§»åŠ¨ç«¯ä¼˜åŒ–',
        'memory_saving': '~75%',
        'precision_loss': 'ä¸­ç­‰'
    },
    'int8_histogram': {
        'precision': 'histogram',
        'backend': 'fbgemm',
        'description': 'INT8ç›´æ–¹å›¾æ ¡å‡†',
        'memory_saving': '~75%',
        'precision_loss': 'è¾ƒå°'
    },
    'int8_moving_avg': {
        'precision': 'moving_average',
        'backend': 'fbgemm',
        'description': 'INT8ç§»åŠ¨å¹³å‡æ ¡å‡†',
        'memory_saving': '~75%',
        'precision_loss': 'è¾ƒå°'
    }
}

def get_quantization_option(option_name):
    """è·å–é¢„å®šä¹‰çš„é‡åŒ–é€‰é¡¹"""
    return STATIC_QUANTIZATION_OPTIONS.get(option_name, STATIC_QUANTIZATION_OPTIONS['int8_default'])


def print_available_quantization_options():
    """æ‰“å°æ‰€æœ‰å¯ç”¨çš„é‡åŒ–é€‰é¡¹"""
    print("\n=== å¯ç”¨çš„é‡åŒ–é…ç½®é€‰é¡¹ ===")
    print(f"{'é€‰é¡¹åç§°':<20} {'æè¿°':<30} {'å†…å­˜èŠ‚çœ':<10} {'ç²¾åº¦æŸå¤±':<10}")
    print("-" * 70)
    
    for name, config in STATIC_QUANTIZATION_OPTIONS.items():
        print(f"{name:<20} {config['description']:<30} {config['memory_saving']:<10} {config['precision_loss']:<10}")
    print()

def fuse_model_modules(model):
    print("âš™ï¸ å¼€å§‹ç®—å­èåˆ...")
    model.eval()
    for module in model.modules():
        if isinstance(module, MBConvBlock):
            if hasattr(module, 'expand_conv'):
                torch.quantization.fuse_modules(module.expand_conv, ['0', '1'], inplace=True)
            if hasattr(module, 'dw_conv'):
                torch.quantization.fuse_modules(module.dw_conv, ['0', '1'], inplace=True)
            if hasattr(module, 'pw_conv'):
                torch.quantization.fuse_modules(module.pw_conv, ['0', '1'], inplace=True)
        elif isinstance(module, DWSepConvBlock):
            if hasattr(module, 'dw_conv'):
                torch.quantization.fuse_modules(module.dw_conv, ['0', '1'], inplace=True)
            if hasattr(module, 'pw_conv'):
                torch.quantization.fuse_modules(module.pw_conv, ['0', '1'], inplace=True)
        # å¯¹ SeDpConvBlock è¿›è¡Œèåˆ
        elif isinstance(module, SeDpConvBlock):
            if hasattr(module, 'dw_conv'):
                torch.quantization.fuse_modules(module.dw_conv, ['0', '1'], inplace=True)
        # å¯¹ DpConvBlock è¿›è¡Œèåˆ
        elif isinstance(module, DpConvBlock):
            if hasattr(module, 'dw_conv'):
                torch.quantization.fuse_modules(module.dw_conv, ['0', '1'], inplace=True)
            if hasattr(module, 'pw_conv'):
                torch.quantization.fuse_modules(module.pw_conv, ['0', '1'], inplace=True) 
        # å¯¹ SeSepConvBlock è¿›è¡Œèåˆ
        elif isinstance(module, SeSepConvBlock):
            if hasattr(module, 'dw_conv'):
                torch.quantization.fuse_modules(module.dw_conv, ['0', '1'], inplace=True)
            if hasattr(module, 'pw_conv'):
                torch.quantization.fuse_modules(module.pw_conv, ['0', '1'], inplace=True)
    print("âœ… ç®—å­èåˆå®Œæˆã€‚")

def fuse_QATmodel_modules(model):
    print("âš™ï¸ å¼€å§‹ç®—å­èåˆ...")
    # å¿…é¡»åœ¨è®­ç»ƒæ¨¡å¼ä¸‹è¿›è¡ŒQATå‡†å¤‡
    model.eval()
    # è·å–æ‰€æœ‰éœ€è¦èåˆçš„æ¨¡å—
    modules_to_fuse = []
    for name, module in model.named_modules():
        if isinstance(module, MBConvBlock):
            # MBConvBlockçš„èåˆ
            if hasattr(module, 'expand_conv') and module.expand_conv is not None:
                expand_conv_0 = f"{name}.expand_conv.0"
                expand_conv_1 = f"{name}.expand_conv.1"
                if expand_conv_0 in dict(model.named_modules()) and expand_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([expand_conv_0, expand_conv_1])
            if hasattr(module, 'dw_conv') and module.dw_conv is not None:
                dw_conv_0 = f"{name}.dw_conv.0"
                dw_conv_1 = f"{name}.dw_conv.1"
                if dw_conv_0 in dict(model.named_modules()) and dw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([dw_conv_0, dw_conv_1])
            if hasattr(module, 'pw_conv') and module.pw_conv is not None:
                pw_conv_0 = f"{name}.pw_conv.0"
                pw_conv_1 = f"{name}.pw_conv.1"
                if pw_conv_0 in dict(model.named_modules()) and pw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([pw_conv_0, pw_conv_1])
                
        elif isinstance(module, DWSepConvBlock):
            # DWSepConvBlockçš„èåˆ
            if hasattr(module, 'dw_conv') and module.dw_conv is not None:
                dw_conv_0 = f"{name}.dw_conv.0"
                dw_conv_1 = f"{name}.dw_conv.1"
                if dw_conv_0 in dict(model.named_modules()) and dw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([dw_conv_0, dw_conv_1])
            if hasattr(module, 'pw_conv') and module.pw_conv is not None:
                pw_conv_0 = f"{name}.pw_conv.0"
                pw_conv_1 = f"{name}.pw_conv.1"
                if pw_conv_0 in dict(model.named_modules()) and pw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([pw_conv_0, pw_conv_1])
        # SeDpConvBlockçš„èåˆ
        elif isinstance(module, SeDpConvBlock):
            if hasattr(module, 'dw_conv') and module.dw_conv is not None:
                dw_conv_0 = f"{name}.dw_conv.0"
                dw_conv_1 = f"{name}.dw_conv.1"
                if dw_conv_0 in dict(model.named_modules()) and dw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([dw_conv_0, dw_conv_1])
        
        # DpConvBlockçš„èåˆ
        elif isinstance(module, DpConvBlock):
            if hasattr(module, 'dw_conv') and module.dw_conv is not None:
                dw_conv_0 = f"{name}.dw_conv.0"
                dw_conv_1 = f"{name}.dw_conv.1"
                if dw_conv_0 in dict(model.named_modules()) and dw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([dw_conv_0, dw_conv_1])
            if hasattr(module, 'pw_conv') and module.pw_conv is not None:
                pw_conv_0 = f"{name}.pw_conv.0"
                pw_conv_1 = f"{name}.pw_conv.1"
                if pw_conv_0 in dict(model.named_modules()) and pw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([pw_conv_0, pw_conv_1])
        
        # SeSepConvBlockçš„èåˆ
        elif isinstance(module, SeSepConvBlock):
            if hasattr(module, 'dw_conv') and module.dw_conv is not None:
                dw_conv_0 = f"{name}.dw_conv.0"
                dw_conv_1 = f"{name}.dw_conv.1"
                if dw_conv_0 in dict(model.named_modules()) and dw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([dw_conv_0, dw_conv_1])
            if hasattr(module, 'pw_conv') and module.pw_conv is not None:
                pw_conv_0 = f"{name}.pw_conv.0"
                pw_conv_1 = f"{name}.pw_conv.1"
                if pw_conv_0 in dict(model.named_modules()) and pw_conv_1 in dict(model.named_modules()):
                    modules_to_fuse.append([pw_conv_0, pw_conv_1])
    
    # æ‰§è¡Œèåˆ
    try:
        if modules_to_fuse:
            torch.quantization.fuse_modules(model, modules_to_fuse, inplace=True)
            print(f"âœ… èåˆäº† {len(modules_to_fuse)} ç»„æ¨¡å—")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°éœ€è¦èåˆçš„æ¨¡å—")
    except Exception as e:
        print(f"âŒ æ¨¡å—èåˆå¤±è´¥: {str(e)}")

def apply_configurable_static_quantization(trained_model, dataloader, precision='int8', backend='fbgemm'):
    """åº”ç”¨å¯é…ç½®çš„é™æ€é‡åŒ–"""
    print(f"ğŸ”§ å¼€å§‹é™æ€é‡åŒ– - ç²¾åº¦: {precision}, åç«¯: {backend}")
    
    # è®¾ç½®é‡åŒ–åç«¯
    torch.backends.quantized.engine = backend
    trained_model.to('cpu').eval()
    
    # èåˆæ¨¡å—
    print("âš™ï¸ èåˆæ¨¡å—...")
    fuse_model_modules(trained_model)
    
    # è·å–é‡åŒ–é…ç½®
    quant_config = get_static_quantization_config(precision)
    print(f"ğŸ“‹ ä½¿ç”¨é…ç½®: {quant_config['description']}")
    
    # åº”ç”¨é‡åŒ–é…ç½®
    trained_model.qconfig = quant_config['qconfig']
    
    # å‡†å¤‡é‡åŒ–
    print("âš™ï¸ å‡†å¤‡é‡åŒ–...")
    quantization.prepare(trained_model, inplace=True)
    
    # æ ¡å‡†é˜¶æ®µ
    print("âš™ï¸ å¼€å§‹æ ¡å‡†...")
    calibration_loader = create_calibration_loader(dataloader['train'], num_batches=12)
    
    trained_model.eval()
    with torch.no_grad():
        for batch_idx, (inputs, _) in enumerate(calibration_loader):
            inputs = inputs.to('cpu')
            if inputs.dtype != torch.float32:
                inputs = inputs.float()
            
            try:
                _ = trained_model(inputs)
            except Exception as e:
                print(f"âš ï¸ æ ¡å‡†æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                continue
                
            if (batch_idx + 1) % 4 == 0:
                print(f"  æ ¡å‡†è¿›åº¦: {batch_idx + 1}/12")
    
    print("âœ… æ ¡å‡†å®Œæˆ")
    
    # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
    print("âš™ï¸ è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹...")
    try:
        quantized_model = quantization.convert(trained_model, inplace=True)
        
        # åˆ†æé‡åŒ–ç»“æœ
        # analyze_quantization_result(quantized_model, precision)
        
        print(f"âœ… {precision} é™æ€é‡åŒ–å®Œæˆ")
        return quantized_model
        
    except Exception as e:
        print(f"âŒ {precision} é‡åŒ–å¤±è´¥: {e}")
        print("ğŸ”„ å›é€€åˆ°é»˜è®¤INT8é‡åŒ–...")
        
        # å›é€€åˆ°é»˜è®¤é…ç½®
        trained_model.qconfig = quantization.get_default_qconfig('fbgemm')
        quantization.prepare(trained_model, inplace=True)
        
        # é‡æ–°æ ¡å‡†
        with torch.no_grad():
            for inputs, _ in calibration_loader:
                inputs = inputs.to('cpu')
                if inputs.dtype != torch.float32:
                    inputs = inputs.float()
                _ = trained_model(inputs)
        
        quantized_model = quantization.convert(trained_model, inplace=True)
        print("âœ… é»˜è®¤é‡åŒ–å®Œæˆ")
        return quantized_model


def analyze_quantization_result(model, precision):
    """åˆ†æé‡åŒ–ç»“æœ"""
    print(f"\n === {precision} é‡åŒ–ç»“æœåˆ†æ === ")
    
    total_params = 0
    quantized_params = 0
    total_memory = 0
    quantized_memory = 0
    
    for name, param in model.named_parameters():
        param_count = param.numel()
        param_memory = param_count * param.element_size()
        
        total_params += param_count
        total_memory += param_memory
        
        if 'qint' in str(param.dtype):
            quantized_params += param_count
            quantized_memory += param_memory
            status = "âœ… å·²é‡åŒ–"
        else:
            status = "âŒ æœªé‡åŒ–"
        
        print(f"{name}:")
        print(f"  å‚æ•°æ•°: {param_count:,}")
        print(f"  ç±»å‹: {param.dtype}")
        print(f"  å†…å­˜: {param_memory/1024/1024:.3f} MB")
        print(f"  çŠ¶æ€: {status}")
        print()
    
    # ç»Ÿè®¡ç»“æœ
    quantization_ratio = quantized_params / total_params * 100 if total_params > 0 else 0
    memory_ratio = quantized_memory / total_memory * 100 if total_memory > 0 else 0
    
    print("=== é‡åŒ–ç»Ÿè®¡ ===")
    print(f"æ€»å‚æ•°æ•°: {total_params:,}")
    print(f"é‡åŒ–å‚æ•°æ•°: {quantized_params:,}")
    print(f"é‡åŒ–æ¯”ä¾‹: {quantization_ratio:.1f}%")
    print(f"æ€»å†…å­˜: {total_memory/1024/1024:.2f} MB")
    print(f"é‡åŒ–åå†…å­˜: {quantized_memory/1024/1024:.2f} MB") 
    print(f"å†…å­˜é‡åŒ–æ¯”ä¾‹: {memory_ratio:.1f}%")
    
    # ä¼°ç®—å†…å­˜èŠ‚çœ
    if total_params > 0:
        fp32_memory = total_params * 4 / 1024 / 1024  # FP32åŸºå‡†
        current_memory = total_memory / 1024 / 1024
        memory_saving = (1 - current_memory / fp32_memory) * 100
        print(f"ç›¸æ¯”FP32å†…å­˜èŠ‚çœ: {memory_saving:.1f}%")
    
    return {
        'quantization_ratio': quantization_ratio,
        'memory_saving': memory_saving if total_params > 0 else 0
    }

def prepare_qaft_model(model, freeze_backbone=True):
    """
    å‡†å¤‡QAFTé‡åŒ–æ„ŸçŸ¥å¾®è°ƒæ¨¡å‹
    
    Args:
        model: é¢„è®­ç»ƒæ¨¡å‹
        freeze_backbone: æ˜¯å¦å†»ç»“éª¨å¹²ç½‘ç»œ (åªå¾®è°ƒé‡åŒ–å‚æ•°)
    
    Returns:
        å‡†å¤‡å¥½çš„QAFTæ¨¡å‹
    """
    print("ğŸ”§ å‡†å¤‡QAFTé‡åŒ–æ„ŸçŸ¥å¾®è°ƒ")
    
    try:
        # 1. è®¾ç½®QAFTé…ç½® (æ¯”QATæ›´ä¿å®ˆçš„é…ç½®)
        model.qconfig = torch.quantization.QConfig(
            activation=torch.quantization.FakeQuantize.with_args(
                observer=torch.quantization.MovingAverageMinMaxObserver,
                quant_min=0,
                quant_max=255,
                dtype=torch.quint8,
                qscheme=torch.per_tensor_affine,
                reduce_range=False
            ),
            weight=torch.quantization.FakeQuantize.with_args(
                observer=torch.quantization.MovingAveragePerChannelMinMaxObserver,
                quant_min=-128,
                quant_max=127,
                dtype=torch.qint8,
                qscheme=torch.per_channel_symmetric,
                reduce_range=False
            )
        )
        
        # 2. èåˆæ¨¡å—
        print("âš™ï¸ èåˆæ¨¡å—...")
        fuse_QATmodel_modules(model)
        
        # 3. å‡†å¤‡QAFT
        model.train()
        torch.quantization.prepare_qat(model, inplace=True)
        
        # 4. å†»ç»“éª¨å¹²ç½‘ç»œ (å¯é€‰)
        if freeze_backbone:
            print("â„ï¸ å†»ç»“éª¨å¹²ç½‘ç»œå‚æ•°")
            for name, param in model.named_parameters():
                # åªè®­ç»ƒFakeQuantizeçš„scaleå’Œzero_point
                if 'activation_post_process' not in name and 'weight_fake_quant' not in name:
                    param.requires_grad = False
                else:
                    param.requires_grad = True
                    print(f"  âœ… ä¿æŒå¯è®­ç»ƒ: {name}")
        
        print("âœ… QAFTå‡†å¤‡å®Œæˆ")
        return model
        
    except Exception as e:
        print(f"âŒ QAFTå‡†å¤‡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return model


def apply_qaft_quantization(pretrained_model, dataloader, fine_tune_epochs=10, 
                            freeze_backbone=True, learning_rate=1e-4):
    """
    åº”ç”¨QAFTé‡åŒ–æ„ŸçŸ¥å¾®è°ƒ
    
    Args:
        pretrained_model: é¢„è®­ç»ƒæ¨¡å‹
        dataloader: æ•°æ®åŠ è½½å™¨
        fine_tune_epochs: å¾®è°ƒè½®æ•°
        freeze_backbone: æ˜¯å¦å†»ç»“éª¨å¹²ç½‘ç»œ
        learning_rate: å­¦ä¹ ç‡
    
    Returns:
        é‡åŒ–åçš„æ¨¡å‹å’Œæ€§èƒ½æŒ‡æ ‡
    """
    import copy
    import torch.nn as nn
    import torch.optim as optim
    
    print(f"ğŸ¯ å¼€å§‹QAFTé‡åŒ–æ„ŸçŸ¥å¾®è°ƒ (å¾®è°ƒ{fine_tune_epochs}è½®)")
    
    try:
        # 1. å¤åˆ¶æ¨¡å‹
        model = copy.deepcopy(pretrained_model)
        model = model.to('cpu')
        
        # 2. å‡†å¤‡QAFT
        model = prepare_qaft_model(model, freeze_backbone=freeze_backbone)
        
        # 3. åˆ›å»ºä»»åŠ¡å¤´
        num_classes = len(dataloader['train'].dataset.classes)
        task_head = nn.Linear(model.output_dim, num_classes).to('cpu')
        
        # 4. è®¾ç½®ä¼˜åŒ–å™¨ (åªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°)
        trainable_params = [p for p in model.parameters() if p.requires_grad]
        head_params = list(task_head.parameters())
        
        optimizer = optim.Adam(
            trainable_params + head_params,
            lr=learning_rate
        )
        criterion = nn.CrossEntropyLoss()
        
        # 5. å¾®è°ƒè®­ç»ƒ
        model.train()
        task_head.train()
        
        print(f"ğŸ“š å¼€å§‹å¾®è°ƒ (å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in trainable_params):,})")
        
        best_accuracy = 0.0
        for epoch in range(fine_tune_epochs):
            running_loss = 0.0
            correct = 0
            total = 0
            
            for batch_idx, (inputs, labels) in enumerate(dataloader['train']):
                inputs = inputs.to('cpu')
                labels = labels.to('cpu')
                
                # å‰å‘ä¼ æ’­
                optimizer.zero_grad()
                features = model(inputs)
                outputs = task_head(features)
                loss = criterion(outputs, labels)
                
                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                
                # ç»Ÿè®¡
                running_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
                
                if (batch_idx + 1) % 50 == 0:
                    print(f"  Epoch {epoch+1}/{fine_tune_epochs}, "
                          f"Batch {batch_idx+1}, "
                          f"Loss: {running_loss/(batch_idx+1):.4f}, "
                          f"Acc: {100.*correct/total:.2f}%")
            
            epoch_acc = 100. * correct / total
            print(f"âœ… Epoch {epoch+1}/{fine_tune_epochs} å®Œæˆ: "
                  f"Loss={running_loss/len(dataloader['train']):.4f}, "
                  f"Acc={epoch_acc:.2f}%")
            
            if epoch_acc > best_accuracy:
                best_accuracy = epoch_acc
        
        # 6. è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
        print("âš™ï¸ è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹...")
        model.eval()
        quantized_model = torch.quantization.convert(model, inplace=False)
        
        # 7. è¯„ä¼°é‡åŒ–æ¨¡å‹
        print("ğŸ“Š è¯„ä¼°é‡åŒ–æ¨¡å‹...")
        task_head.eval()
        quantized_model.eval()
        
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, labels in dataloader['test']:
                inputs = inputs.to('cpu')
                labels = labels.to('cpu')
                
                features = quantized_model(inputs)
                if not isinstance(features, torch.Tensor):
                    features = features.dequantize()
                
                outputs = task_head(features)
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        qaft_accuracy = 100. * correct / total
        print(f"ğŸ¯ QAFTé‡åŒ–å‡†ç¡®ç‡: {qaft_accuracy:.2f}%")
        
        # 8. æµ‹é‡æ€§èƒ½
        import time
        from utils import calculate_memory_usage
        
        dummy_input = torch.randn(64, dataloader['train'].dataset[0][0].shape[0], 
                                  dataloader['train'].dataset[0][0].shape[1], 
                                  device='cpu')
        
        # æµ‹å»¶è¿Ÿ
        repetitions = 50
        timings = []
        with torch.no_grad():
            for i in range(repetitions):
                start = time.time()
                _ = quantized_model(dummy_input)
                end = time.time()
                if i >= 10:
                    timings.append((end - start) * 1000)
        
        latency_ms = sum(timings) / len(timings) if timings else 0
        
        # æµ‹å†…å­˜
        memory_usage = calculate_memory_usage(
            quantized_model,
            input_size=tuple(dummy_input.shape),
            device='cpu'
        )
        
        metrics = {
            'accuracy': qaft_accuracy,
            'latency': latency_ms,
            'activation_memory': memory_usage['activation_memory_MB'],
            'parameter_memory': memory_usage['parameter_memory_MB'],
            'peak_memory': memory_usage['total_memory_MB']
        }
        
        print(f"âœ… QAFTå®Œæˆ: å‡†ç¡®ç‡={qaft_accuracy:.2f}%, "
              f"å»¶è¿Ÿ={latency_ms:.2f}ms, "
              f"å†…å­˜={memory_usage['total_memory_MB']:.2f}MB")
        
        return quantized_model, metrics
        
    except Exception as e:
        print(f"âŒ QAFTé‡åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

class QuantizableModel(torch.nn.Module):
    """
    å¯é‡åŒ–æ¨¡å‹åŒ…è£…å™¨ï¼Œæ·»åŠ é‡åŒ–stub
    """
    def __init__(self, model):
        super(QuantizableModel, self).__init__()
        self.model = model
        self.quant = QuantStub()
        self.dequant = DeQuantStub()
        self.quant_mode = None  # æ·»åŠ é‡åŒ–æ¨¡å¼æ ‡å¿—
        self.quant_enabled = True
        
    def enable_quant(self):
        self.quant_enabled = True
        
    def disable_quant(self):
        self.quant_enabled = False

    def set_quant_mode(self, mode):
        self.quant_mode = mode
    
    def forward(self, x):
        if self.quant_mode in ['dynamic', 'static', 'qat']:  # ä»…åœ¨é‡åŒ–æ¨¡å¼ä¸‹æ‰§è¡Œ
            x = self.quant(x)
            x = self.model(x)
            x = self.dequant(x)
        else:
            x = self.model(x)
        return x


    # æ·»åŠ ä»¥ä¸‹æ–¹æ³•ä»¥ä¿æŒä¸åŸå§‹æ¨¡å‹çš„å…¼å®¹æ€§
    @property
    def output_dim(self):
        return self.model.output_dim

    # æ·»åŠ çŠ¶æ€å­—å…¸å¤„ç†
    def load_state_dict(self, state_dict, strict=True):
        # å…ˆå°è¯•ç›´æ¥åŠ è½½
        try:
            super().load_state_dict(state_dict, strict)
        except RuntimeError as e:
            if "Missing key(s)" in str(e):
                # ä¿®å¤çŠ¶æ€å­—å…¸é”®å
                new_state_dict = {}
                for k, v in state_dict.items():
                    if k.startswith('model.'):
                        new_state_dict[k[6:]] = v
                    else:
                        new_state_dict[k] = v
                super().load_state_dict(new_state_dict, strict=False)
            else:
                raise e
            
    # æ·»åŠ é™æ€é‡åŒ–æ”¯æŒ
    def prepare_static_quantization(self):
        """
        å‡†å¤‡é™æ€é‡åŒ–ï¼šä¸ºæ¨¡å‹æ’å…¥è§‚å¯Ÿå™¨ã€‚
        """
        self.eval()  # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
        self.qconfig = torch.quantization.get_default_qconfig('fbgemm')  #  fbgemm æˆ– 'qnnpack'
        # ç‰¹åˆ«å¤„ç†GroupNormå±‚
        torch.quantization.quantize_default_mappings[torch.nn.GroupNorm] = torch.quantization.default_float_to_quantized_operator_mappings[torch.nn.GroupNorm]
        torch.quantization.prepare(self, inplace=True)  # æ’å…¥è§‚å¯Ÿå™¨
        print("é‡åŒ–å‡†å¤‡åçš„æ¨¡å‹ç»“æ„:")
        print(self)  # æ‰“å°æ¨¡å‹ç»“æ„ï¼Œæ£€æŸ¥è§‚å¯Ÿå™¨æ˜¯å¦æ­£ç¡®æ’å…¥

    def convert_static_quantization(self):
        """
        è½¬æ¢ä¸ºé™æ€é‡åŒ–æ¨¡å‹
        """
        quantization.convert(self, inplace=True)

