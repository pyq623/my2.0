import json
import random
import numpy as np
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent))  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
from typing import List, Dict, Any, Tuple
import os
import threading
import queue
import time
from datetime import datetime
import pytz
from data import get_multitask_dataloaders, get_dataset_info
from concurrent.futures import ThreadPoolExecutor, as_completed
from multiprocessing import Process, Manager, Queue
from training import SingleTaskTrainer
import torch
from models.candidate_models import CandidateModel
from nas import evaluate_quantized_model
from models import apply_configurable_static_quantization, get_quantization_option, fuse_QATmodel_modules
import multiprocessing as mp
import signal
import logging
from collections import defaultdict
import copy
import argparse


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="åŠ¨æ€ç”Ÿæˆæ¶æ„æ¨¡å‹å¹¶è¿›è¡Œè®­ç»ƒå’Œé‡åŒ–")
    parser.add_argument(
        "--dataset_name", 
        type=str, 
        required=True, 
        help="æŒ‡å®šè¦ä½¿ç”¨çš„æ•°æ®é›†åç§°ï¼Œä¾‹å¦‚ UTD-MHAD æˆ–å…¶ä»–æ•°æ®é›†"
    )


    return parser.parse_args()

# è®¾ç½®æ—¥å¿—
def setup_logger(gpu_id, log_dir):
    """ ä¸ºæ¯ä¸ªGPUè¿›ç¨‹è®¾ç½®å•ç‹¬çš„æ—¥å¿—æ–‡ä»¶ """
    os.makedirs(log_dir, exist_ok=True)
    logger = logging.getLogger(f'GPU_{gpu_id}')
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤ç°æœ‰çš„å¤„ç†å™¨
    if logger.handlers:
        logger.handlers.clear()
    
    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(os.path.join(log_dir, f'output_{gpu_id}.log'))
    file_handler.setLevel(logging.INFO)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # æ ¼å¼åŒ–
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

def _load_dataset_info( name: str) -> Dict[str, Any]:
    """åŠ è½½æ•°æ®é›†ä¿¡æ¯"""
    return get_dataset_info(name)
# self.dataset_info = self._load_dataset_info(name)
#  num_classes = self.dataset_info[dataset_name]['num_classes']
# input_size=(64, self.dataset_info[dataset_name]['channels'], 
                        # self.dataset_info[dataset_name]['time_steps'])
def set_random_seed(seed=2002):
    """è®¾ç½®æ‰€æœ‰éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def _prepare_model_for_qat(model, device):
    """ä¸ºQATé‡åŒ–æ„ŸçŸ¥è®­ç»ƒå‡†å¤‡æ¨¡å‹"""
    try:
        print("âš™ï¸ è®¾ç½®QATé…ç½®å’Œèåˆæ¨¡å—")
        
        # è®¾ç½®QATé…ç½®
        model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
        
        fuse_QATmodel_modules(model)
        # å‡†å¤‡QAT
        # ç¡®ä¿æ¨¡å‹å¤„äºè®­ç»ƒæ¨¡å¼
        model.train()
        model.to(device)
        torch.quantization.prepare_qat(model, inplace=True)
        print("âœ… QATå‡†å¤‡å®Œæˆ")
        
        return model
        
    except Exception as e:
        print(f"âŒ QATå‡†å¤‡å¤±è´¥: {str(e)}")
        return model  # è¿”å›åŸå§‹æ¨¡å‹

def _apply_quantization_helper(model, dataloader, quant_mode: str, quantization_option: str = 'int8_per_channel'):
    """é‡åŒ–è¾…åŠ©æ–¹æ³•ï¼Œå¤ç”¨åŸæœ‰é€»è¾‘"""
    # è¿™é‡Œç›´æ¥è°ƒç”¨ä½ åŸæœ‰çš„apply_quantizationæ–¹æ³•
    # éœ€è¦ç¨å¾®ä¿®æ”¹ä»¥é€‚åº”æ–°çš„æ¥å£
    import copy
    model_copy = copy.deepcopy(model)
    
    if quant_mode == 'dynamic':
        model_copy.to('cpu').eval()
        quantized_model = torch.quantization.quantize_dynamic(
            model_copy,
            {torch.nn.Conv1d, torch.nn.Linear},
            dtype=torch.qint8
        )
    elif quant_mode == 'static':
        # int8_default  int8_per_channel int8_reduce_range
        quant_config = get_quantization_option(quantization_option)
        print(f"ğŸ“‹ é€‰æ‹©é‡åŒ–é…ç½®: {quant_config['description']}")
        quantized_model = apply_configurable_static_quantization(
            model_copy,
            dataloader,
            precision=quant_config['precision'],
            backend=quant_config['backend']
        )
    elif quant_mode == 'qat':
        # QATè®­ç»ƒååªéœ€è¦è½¬æ¢ï¼Œä¸éœ€è¦å°è¯•ä¸åŒé€‰é¡¹
        # QATè®­ç»ƒåè½¬æ¢
        print("ğŸ”§ è½¬æ¢QATæ¨¡å‹ä¸ºé‡åŒ–æ¨¡å‹")
        model_copy.eval()
        model_copy.to('cpu')  # å°†æ¨¡å‹ç§»åŠ¨åˆ°CPU
        quantized_model = torch.quantization.convert(model_copy, inplace=False)
        print("âœ… QATè½¬æ¢å®Œæˆ")
    else:
        return model
    
    return quantized_model

def train_qat_model(model, dataloader, device, save_path, logger, epochs=100):
    """è®­ç»ƒQATæ¨¡å‹ - ä»å¤´å¼€å§‹è®­ç»ƒæœªç»è®­ç»ƒçš„æ¨¡å‹"""
    try:
        logger.info("ğŸ‹ï¸ å¼€å§‹ QAT é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ")
        
        # å‡†å¤‡ QAT æ¨¡å‹
        qat_model = _prepare_model_for_qat(copy.deepcopy(model), device)
        
        # åˆ›å»ºQATè®­ç»ƒå™¨
        qat_trainer = SingleTaskTrainer(qat_model, dataloader, device=device, logger=logger)
        
        # è®­ç»ƒQATæ¨¡å‹ï¼ˆå¯ä»¥ä½¿ç”¨è¾ƒå°‘çš„ epoch ï¼Œå› ä¸ºåŸºç¡€æ¨¡å‹å·²ç»è®­ç»ƒè¿‡ï¼‰
        best_acc, best_val_metrics, history, best_state = qat_trainer.train(
            epochs=epochs, save_path=save_path
        )
        
        logger.info(f"âœ… QAT è®­ç»ƒå®Œæˆ - Acc: {best_acc:.2f}%")
        return qat_model, best_acc, best_state
        
    except Exception as e:
        logger.error(f"âŒ QATè®­ç»ƒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, 0.0, None


def test_model_worker(config, description, dataset_name, base_save_dir, gpu_id, result_queue, logger):
    """å·¥ä½œè¿›ç¨‹å‡½æ•°ï¼Œåœ¨æŒ‡å®šçš„GPUä¸Šæµ‹è¯•æ¨¡å‹"""
    try:
        worker_seed = 2002 + gpu_id
        set_random_seed(worker_seed)
        torch.cuda.set_device(gpu_id)
        device = torch.device(f'cuda:{gpu_id}')
        
        # print(f"ğŸš€ è¿›ç¨‹ {os.getpid()} åœ¨ GPU {gpu_id} ä¸Šæµ‹è¯•: {description}")
        logger.info(f"ğŸš€ è¿›ç¨‹ {os.getpid()} åœ¨ GPU {gpu_id} ä¸Šæµ‹è¯•: {description}")
        
        dataset_info = _load_dataset_info(dataset_name)
        dataloaders = get_multitask_dataloaders('/root/tinyml/data')
        dataloader = dataloaders[dataset_name]
        
        candidate = CandidateModel(config=config)
        model = candidate.build_model().to(device)

        # print(f"ğŸ“Š GPU {gpu_id} ä»£ç†åˆ†æ•°è®¡ç®—å®Œæˆ: {description}")
        logger.info(f"ğŸ“Š GPU {gpu_id} æ¨¡å‹æè¿°: {description}")
        trainer = SingleTaskTrainer(model, dataloader, device=device, logger=logger)

        # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
        model_save_dir = os.path.join(base_save_dir, description.replace(" ", "_"))
        os.makedirs(model_save_dir, exist_ok=True)
        # åŸå§‹è·¯å¾„
        original_model_save_path  = os.path.join(model_save_dir, "best_model.pth")

        # 1. è®­ç»ƒåŸå§‹æ¨¡å‹
        logger.info(f"ğŸ‹ï¸ GPU {gpu_id} å¼€å§‹è®­ç»ƒåŸå§‹æ¨¡å‹: {description} (100 epochs)")
        best_acc, best_val_metrics, history, best_state = trainer.train(
            epochs=100, save_path=original_model_save_path
        )
        
        result = {
            "description": description,
            "accuracy": best_acc,
            "val_accuracy": best_val_metrics['accuracy'] / 100,
            "config": config,
            "gpu_id": gpu_id,
            "status": "success",
        }
        
        # ä¿å­˜åŸå§‹æ¨¡å‹é…ç½®
        config_save_path = os.path.join(model_save_dir, "model.json")
        model_data = {
            "config": config,
            "accuracy": best_acc,
            "val_accuracy": result["val_accuracy"],
            "gpu_id": gpu_id
        }

        def convert_numpy_types(obj):
            if isinstance(obj, (np.integer, np.floating)):
                return float(obj) if isinstance(obj, np.floating) else int(obj)
            elif isinstance(obj, np.bool_):
                return bool(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_numpy_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            return obj
        
        with open(config_save_path, "w", encoding="utf-8") as f:
            converted_data = convert_numpy_types(model_data)
            json.dump(converted_data, f, indent=2, ensure_ascii=False)
        
        result_queue.put(result)
        # print(f"âœ… GPU {gpu_id} è®­ç»ƒå®Œæˆ: {description} - Acc: {best_acc:.2f}%")
        logger.info(f"âœ… GPU {gpu_id} åŸå§‹æ¨¡å‹è®­ç»ƒå®Œæˆ: {description} - Acc: {best_acc:.2f}%")

        # é™æ€é‡åŒ–éƒ¨åˆ†
        quant_mode = "static"
        quantization_options = [
            ('int8_default', 'é»˜è®¤INT8é‡åŒ–'),
            ('int8_per_channel', 'é€é€šé“INT8é‡åŒ–'), 
            ('int8_reduce_range', 'å‡å°‘èŒƒå›´INT8é‡åŒ–'),
            ('int8_asymmetric', 'INT8éå¯¹ç§°é‡åŒ–'),
            ('int8_histogram', 'INT8ç›´æ–¹å›¾æ ¡å‡†'),
            ('int8_moving_avg', 'INT8ç§»åŠ¨å¹³å‡æ ¡å‡†')
        ]
        
        best_quant_accuracy = 0.0
        best_quantized_model = None
        best_option_name = ""

        for option_name, option_desc in quantization_options:
            try:
                # print(f"ğŸ”¬ å°è¯• {option_desc} ({option_name})")
                logger.info(f"ğŸ”¬ å°è¯• {option_desc} ({option_name})")
                quantized_model = _apply_quantization_helper(
                    model, dataloader, quant_mode, option_name
                )
                if quantized_model:
                    # åˆ›å»ºä»»åŠ¡å¤´å¹¶åŠ è½½æƒé‡
                    task_head = torch.nn.Linear(model.output_dim, 
                        len(dataloader['test'].dataset.classes)).to('cpu')
                    if best_state and 'head' in best_state:
                        task_head.load_state_dict(best_state['head'])
                    
                    # è¯„ä¼°é‡åŒ–æ¨¡å‹å‡†ç¡®ç‡
                    quant_accuracy = evaluate_quantized_model(
                        quantized_model, dataloader, task_head, f" MCTS é‡åŒ–æ¨¡å‹({option_name})"
                    )
                    
                    # print(f"ğŸ“Š {option_desc} ç»“æœ: "
                    #     f"å‡†ç¡®ç‡={quant_accuracy:.1f}%, ")
                    logger.info(f"ğŸ“Š {option_desc} ç»“æœ: å‡†ç¡®ç‡={quant_accuracy:.1f}%")

                    
                    # è®°å½•æœ€ä½³ç»“æœ
                    if quant_accuracy > best_quant_accuracy:
                        best_quant_accuracy = quant_accuracy
                        best_quantized_model = quantized_model
                        best_option_name = option_name
                        
            except Exception as e:
                # print(f"âŒ {option_desc} å¤±è´¥: {str(e)}")
                logger.error(f"âŒ {option_desc} å¤±è´¥: {str(e)}")
                continue

        # ä¿å­˜æœ€ä½³é‡åŒ–æ¨¡å‹
        if best_quantized_model:
            quant_model_save_path = os.path.join(model_save_dir, "quant_best_model.pth")
            quant_config_save_path = os.path.join(model_save_dir, "quant_model.json")
            
            # ä¿å­˜é‡åŒ–æ¨¡å‹æƒé‡
            torch.save(best_quantized_model.state_dict(), quant_model_save_path)
            
            quant_result = {
                "description": f"{description}  (Static Quantized)",
                "accuracy": best_quant_accuracy,
                "quantization_method": best_option_name,
                "config": config,
                "gpu_id": gpu_id,
                "status": "success"
            }

            # ä¿å­˜é‡åŒ–æ¨¡å‹é…ç½®
            quant_model_data = {
                "config": config,
                "quantized_accuracy": best_quant_accuracy,
                "quantization_method": best_option_name
            }
            with open(quant_config_save_path, "w", encoding="utf-8") as f:
                json.dump(convert_numpy_types(quant_model_data), f, indent=2, ensure_ascii=False)
            
            # print(f"ğŸ† é€‰æ‹©æœ€ä½³é‡åŒ–ç®—æ³•: {best_option_name}")
            # print(f"âœ… æœ€ç»ˆé‡åŒ–ç»“æœ: å‡†ç¡®ç‡={best_quant_accuracy:.1f}%")
            logger.info(f"ğŸ† é€‰æ‹©æœ€ä½³é‡åŒ–ç®—æ³•: {best_option_name}")
            logger.info(f"âœ… æœ€ç»ˆé‡åŒ–ç»“æœ: å‡†ç¡®ç‡={best_quant_accuracy:.1f}%")
        
        
        result_queue.put(quant_result)
        # print(f"âœ… é‡åŒ–æ¨¡å‹ GPU {gpu_id} å®Œæˆ: {description} - Acc: {best_acc:.2f}%")
        logger.info(f"âœ… é™æ€é‡åŒ–æ¨¡å‹ GPU {gpu_id} å®Œæˆ: {description} - Acc: {best_acc:.2f}%")
        
        # 3. QAT é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ - ä½¿ç”¨å…¨æ–°çš„æœªç»è®­ç»ƒçš„æ¨¡å‹
        logger.info(f"ğŸ”§ GPU {gpu_id} å¼€å§‹QATé‡åŒ–æ„ŸçŸ¥è®­ç»ƒ: {description}")
        qat_model_save_path = os.path.join(model_save_dir, "qat_best_model.pth")
        qat_config_save_path = os.path.join(model_save_dir, "qat_model.json")

        # åˆ›å»ºæ–°çš„æœªç»è®­ç»ƒçš„æ¨¡å‹ç”¨äºQAT
        candidate = CandidateModel(config=config)
        qat_model = candidate.build_model().to(device)
        # è®­ç»ƒ QAT æ¨¡å‹
        qat_model, qat_accuracy, qat_best_state = train_qat_model(
            qat_model, dataloader, device, qat_model_save_path, logger, epochs=100
        )

        if qat_model:
            # è½¬æ¢QATæ¨¡å‹ä¸ºé‡åŒ–æ¨¡å‹
            logger.info("ğŸ”§ è½¬æ¢ QAT æ¨¡å‹ä¸ºé‡åŒ–æ¨¡å‹")
            qat_model.eval()
            qat_model.to('cpu')
            quantized_qat_model = torch.quantization.convert(qat_model, inplace=False)
            
            # è¯„ä¼° QAT é‡åŒ–æ¨¡å‹
            task_head = torch.nn.Linear(model.output_dim, 
                len(dataloader['test'].dataset.classes)).to('cpu')
            if qat_best_state and 'head' in qat_best_state:
                task_head.load_state_dict(qat_best_state['head'])
            
            qat_quant_accuracy = evaluate_quantized_model(
                quantized_qat_model, dataloader, task_head, f"QATé‡åŒ–æ¨¡å‹"
            )
            
            # ä¿å­˜ QAT é‡åŒ–æ¨¡å‹
            torch.save(quantized_qat_model.state_dict(), qat_model_save_path)
            
            qat_result = {
                "description": f"{description} (QAT Quantized)",
                "accuracy": qat_quant_accuracy,
                "quantization_method": "qat",
                "config": config,
                "gpu_id": gpu_id,
                "status": "success",
            }
            
            # ä¿å­˜ QAT æ¨¡å‹é…ç½®
            qat_model_data = {
                "config": config,
                "qat_accuracy": qat_accuracy,
                "qat_quantized_accuracy": qat_quant_accuracy,
                "quantization_method": "qat",
            }
            with open(qat_config_save_path, "w", encoding="utf-8") as f:
                json.dump(convert_numpy_types(qat_model_data), f, indent=2, ensure_ascii=False)
            
            result_queue.put(qat_result)
            logger.info(f"âœ… QATé‡åŒ–å®Œæˆ: {description} - QAT Acc: {qat_accuracy:.2f}%, Quantized Acc: {qat_quant_accuracy:.2f}%")
        else:
            logger.error(f"âŒ QATè®­ç»ƒå¤±è´¥: {description}")
        
        logger.info(f"âœ… æ‰€æœ‰é‡åŒ–å®Œæˆ GPU {gpu_id}: {description}")

    except Exception as e:
        error_result = {
            "description": description,
            "config": config,
            "status": "failed",
            "error": str(e),
            "gpu_id": gpu_id
        }
        result_queue.put(error_result)
        # print(f"âŒ GPU {gpu_id} å¤±è´¥: {description} - {e}")
        logger.error(f"âŒ GPU {gpu_id} å¤±è´¥: {description} - {e}")
        import traceback
        traceback.print_exc()

def gpu_worker(gpu_id, task_queue, result_queue, dataset_name, base_save_dir, log_dir):
    """GPUå·¥ä½œè¿›ç¨‹ï¼Œä»ä»»åŠ¡é˜Ÿåˆ—è·å–ä»»åŠ¡å¹¶æ‰§è¡Œ"""
    logger = setup_logger(gpu_id, log_dir)
    logger.info(f"ğŸ”„ GPUå·¥ä½œè¿›ç¨‹ {os.getpid()} å¯åŠ¨ï¼Œä½¿ç”¨ GPU {gpu_id}")
    # print(f"ğŸ”„ GPUå·¥ä½œè¿›ç¨‹ {os.getpid()} å¯åŠ¨ï¼Œä½¿ç”¨ GPU {gpu_id}")
    
    while True:
        try:
            task = task_queue.get(timeout=300)
            if task is None:
                # print(f"ğŸ›‘ GPU {gpu_id} æ”¶åˆ°ç»“æŸä¿¡å·")
                logger.info(f"ğŸ›‘ GPU {gpu_id} æ”¶åˆ°ç»“æŸä¿¡å·")
                break
                
            config, description = task
            test_model_worker(config, description, dataset_name, base_save_dir, gpu_id, result_queue, logger)
            
        except queue.Empty:
            logger.info(f"â° GPU {gpu_id} ç­‰å¾…ä»»åŠ¡è¶…æ—¶ï¼Œé€€å‡º")
            break
        except Exception as e:
            # print(f"âŒ GPU {gpu_id} å·¥ä½œè¿›ç¨‹é”™è¯¯: {e}")
            logger.error(f"âŒ GPU {gpu_id} å·¥ä½œè¿›ç¨‹é”™è¯¯: {e}")
            break


class ArchitectureGenerator:
    def __init__(self, search_space: Dict[str, Any], dataset_name: str = 'UTD-MHAD', seed=2002):
        self.search_space = search_space
        self.dataset_name = dataset_name
        self.dataset_info = _load_dataset_info(dataset_name)
        self.seed = seed
        set_random_seed(seed)
        self.lock = threading.Lock()  # çº¿ç¨‹é”
        
    def generate_random_config(self) -> Dict[str, Any]:
        """ç”Ÿæˆä¸€ä¸ªå®Œå…¨éšæœºçš„æ¶æ„é…ç½®"""
        # ä»æ•°æ®é›†ä¿¡æ¯è·å–è¾“å…¥é€šé“æ•°å’Œç±»åˆ«æ•°
        input_channels = self.dataset_info['channels']
        num_classes = self.dataset_info['num_classes']

        # éšæœºé€‰æ‹© stage æ•°é‡
        num_stages = random.choice(self.search_space['stages'])
        
        stages = []
        previous_channels = input_channels    # è¾“å…¥é€šé“æ•°
        
        for stage_idx in range(num_stages):
            stage_config = self._generate_stage_config(stage_idx, previous_channels)
            stages.append(stage_config)
            previous_channels = stage_config['channels']
        
        config = {
            "input_channels": input_channels,
            "num_classes": num_classes,
            "quant_mode": "none",  # å›ºå®šä¸º none
            "stages": stages,
            "constraints": self.search_space.get('constraints', {})
        }
        
        return config
    
    def _generate_stage_config(self, stage_idx: int, previous_channels: int) -> Dict[str, Any]:
        """ç”Ÿæˆå•ä¸ª stage çš„é…ç½®"""
        # éšæœºé€‰æ‹© block æ•°é‡
        num_blocks = random.choice(self.search_space['blocks_per_stage'])
        
        blocks = []
        has_se_dp_conv = False
        for block_idx in range(num_blocks):
            block_config = self._generate_block_config(stage_idx, block_idx, previous_channels)
            blocks.append(block_config)
            if block_config['type'] == "SeDpConv" or block_config['type'] == "DpConv":
                has_se_dp_conv = True

        # å¦‚æœæœ‰SeDpConvæˆ–DpConvï¼Œåˆ™é€šé“æ•°å¿…é¡»ç­‰äºè¾“å…¥é€šé“æ•°
        if has_se_dp_conv:
            channels = previous_channels
        else:
            # éšæœºé€‰æ‹©é€šé“æ•°
            channels = random.choice(self.search_space['channels'])
        
        return {
            "blocks": blocks,
            "channels": channels
        }
    
    def _generate_block_config(self, stage_idx: int, block_idx: int, previous_channels: int) -> Dict[str, Any]:
        """ç”Ÿæˆå•ä¸ªblockçš„é…ç½®"""
        conv_type = random.choice(self.search_space['conv_types'])
        
        # æ ¹æ®å·ç§¯ç±»å‹è®¾ç½®é»˜è®¤å‚æ•°
        if conv_type == "MBConv":
            expansion = random.choice([x for x in self.search_space['expansions'] if x > 1])
            has_se = random.choice(self.search_space['has_se'])
            skip_connection = random.choice(self.search_space['skip_connection']) if stage_idx > 0 else False
        elif conv_type == "DWSepConv":
            expansion = 1
            has_se = random.choice(self.search_space['has_se'])
            skip_connection = random.choice(self.search_space['skip_connection']) if stage_idx > 0 else False
        elif conv_type == "DpConv":
            expansion = 1
            has_se = False
            skip_connection = False
        elif conv_type == "SeSepConv":
            expansion = 1
            has_se = random.choice(self.search_space['has_se'])
            skip_connection = False
        elif conv_type == "SeDpConv":
            expansion = 1
            has_se = random.choice(self.search_space['has_se'])
            skip_connection = False
            # SeDpConvåœ¨ç¬¬ä¸€å±‚çš„é€šé“å¿…é¡»ä¸è¾“å…¥é€šé“æ•°ç›¸åŒ
            if stage_idx == 0 and block_idx == 0:
                previous_channels = self.dataset_info['channels']
        
        # è®¾ç½®SEæ¯”ä¾‹
        se_ratio = random.choice(self.search_space['se_ratios']) if has_se else 0
        
        # éšæœºé€‰æ‹©å…¶ä»–å‚æ•°
        kernel_size = random.choice(self.search_space['kernel_sizes'])
        stride = random.choice(self.search_space['strides'])
        activation = random.choice(self.search_space['activations'])
        
        block_config = {
            "type": conv_type,
            "kernel_size": kernel_size,
            "expansion": expansion,
            "has_se": has_se,
            "se_ratios": se_ratio,
            "skip_connection": skip_connection,
            "stride": stride,
            "activation": activation
        }
        
        return block_config
    
    def _generate_configs_worker(self, stage_count: int, target_count: int, 
                               seen_configs: set, result_queue: queue.Queue,
                               worker_id: int):
        """å·¥ä½œçº¿ç¨‹å‡½æ•°ï¼š ç”Ÿæˆå›ºå®š stage æ•°é‡çš„é…ç½®"""
        worker_configs = []
        worker_seen = set()
        attempts = 0
        max_attempts = target_count * 5  # é˜²æ­¢æ— é™å¾ªç¯
        
        print(f"ğŸ§µ å·¥ä½œçº¿ç¨‹ {worker_id} å¼€å§‹ç”Ÿæˆ {target_count} ä¸ª {stage_count} stage é…ç½®")
        
        while len(worker_configs) < target_count and attempts < max_attempts:
            attempts += 1
            
            config = self.generate_random_config()
            # ç¡®ä¿ stage æ•°é‡æ­£ç¡®
            if len(config['stages']) != stage_count:
                continue
            
            config_hash = self._get_config_hash(config)
            if config_hash in worker_seen or config_hash in seen_configs:
                continue
            
            worker_seen.add(config_hash)
            
            description = self._generate_description(config)
            worker_configs.append((config, description))

            # æ¯ç”Ÿæˆ100ä¸ªé…ç½®æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if len(worker_configs) % 100 == 0:
                print(f"  ğŸ§µ çº¿ç¨‹ {worker_id}: å·²ç”Ÿæˆ {len(worker_configs)}/{target_count} ä¸ª {stage_count} stage é…ç½®")
        
        # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—
        with self.lock:
            seen_configs.update(worker_seen)
        
        result_queue.put((worker_id, stage_count, worker_configs))
        print(f"âœ… å·¥ä½œçº¿ç¨‹ {worker_id} å®Œæˆ: ç”Ÿæˆ {len(worker_configs)} ä¸ª {stage_count} stage é…ç½®")

    def generate_stratified_configs(self, num_configs: int, num_threads: int = 4) -> List[Tuple[Dict[str, Any], str]]:
        """ä½¿ç”¨åˆ†å±‚æŠ½æ ·ç­–ç•¥ç”Ÿæˆé…ç½®ï¼Œç¡®ä¿å¤šæ ·æ€§"""
        configurations = []
        seen_configs = set()
        
        # æŒ‰stageæ•°é‡åˆ†å±‚ï¼Œä½¿ç”¨æŒ‡æ•°åˆ†å¸ƒåˆ†é…
        stage_counts = self.search_space['stages']
        stage_targets = self._calculate_exponential_targets(stage_counts, num_configs)

        print("ğŸ“Š Stage æ•°é‡åˆ†é…ç­–ç•¥:")
        for stage_count, target in stage_targets.items():
            print(f"  Stage {stage_count}: {target} ä¸ªé…ç½®")
        
        # ä¸ºæ¯ä¸ª stage æ•°é‡åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
        result_queue = queue.Queue()
        threads = []

        for stage_count, total_target in stage_targets.items():
            # å°†ç›®æ ‡æ•°é‡ åˆ†é… ç»™å„ä¸ªçº¿ç¨‹
            targets_per_thread = self._distribute_targets(total_target, num_threads)
            
            for thread_id, thread_target in enumerate(targets_per_thread):
                if thread_target > 0:
                    thread = threading.Thread(
                        target=self._generate_configs_worker,
                        args=(stage_count, thread_target, seen_configs, result_queue, thread_id),
                        name=f"Stage{stage_count}_Worker{thread_id}"
                    )
                    threads.append(thread)
        
        # å¯åŠ¨æ‰€æœ‰çº¿ç¨‹
        print(f"ğŸš€ å¯åŠ¨ {len(threads)} ä¸ªå·¥ä½œçº¿ç¨‹...")
        for thread in threads:
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        # æ”¶é›†ç»“æœ
        while not result_queue.empty():
            worker_id, stage_count, worker_configs = result_queue.get()
            configurations.extend(worker_configs)
            print(f"ğŸ“¦ ä»çº¿ç¨‹ {worker_id} æ”¶é›†åˆ° {len(worker_configs)} ä¸ª Stage {stage_count} é…ç½®")
        
        # æ£€æŸ¥é‡å¤é…ç½®
        unique_configs = set()
        duplicate_count = 0
        
        for config, description in configurations:
            config_hash = self._get_config_hash(config)
            if config_hash in unique_configs:
                duplicate_count += 1
            else:
                unique_configs.add(config_hash)
        
        print(f"ğŸ” é…ç½®å»é‡æ£€æŸ¥: æ€»é…ç½®æ•° {len(configurations)}, å”¯ä¸€é…ç½®æ•° {len(unique_configs)}, é‡å¤é…ç½®æ•° {duplicate_count}")

        return configurations
    
    def _distribute_targets(self, total_target: int, num_threads: int) -> List[int]:
        """å°†ç›®æ ‡æ•°é‡åˆ†é…ç»™å„ä¸ªçº¿ç¨‹"""
        base_target = total_target // num_threads
        remainder = total_target % num_threads
        
        targets = [base_target] * num_threads
        for i in range(remainder):
            targets[i] += 1
        
        return targets
    
    def _calculate_exponential_targets(self, categories: List[Any], total: int) -> Dict[Any, int]:
        """è®¡ç®—æŒ‡æ•°åˆ†å¸ƒçš„åˆ†å±‚æŠ½æ ·ç›®æ ‡æ•°é‡"""
        # è®¡ç®—æ¯ä¸ª stage æ•°é‡çš„ç»„åˆå¤æ‚åº¦æƒé‡
        # stage æ•°é‡è¶Šå¤šï¼Œå¯èƒ½çš„ç»„åˆè¶Šå¤šï¼Œåº”è¯¥åˆ†é…æ›´å¤šçš„é…ç½®
        weights = {}
        max_stage = max(categories)
        
        # ä½¿ç”¨æŒ‡æ•°æƒé‡ï¼š stage æ•°é‡ä¸º n çš„ æƒé‡ä¸º base^(n-1)
        base = 4  # æ¯ä¸ª stage å¢åŠ ï¼Œç»„åˆæ•°é‡å¤§çº¦å¢åŠ 4å€
        
        for stage_count in categories:
            # stageæ•°é‡ä¸ºnçš„æƒé‡ä¸º base ^ (n - 1)
            weights[stage_count] = base ** (stage_count - 1)
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(weights.values())
        
        targets = {}
        remaining = total
        
        # æŒ‰æƒé‡åˆ†é…ï¼Œä½†ç¡®ä¿æ¯ä¸ª stage è‡³å°‘æœ‰ä¸€ä¸ªé…ç½®
        for stage_count in sorted(categories):
            if stage_count == max_stage:
                # æœ€åä¸€ä¸ªstageåˆ†é…å‰©ä½™çš„æ‰€æœ‰
                targets[stage_count] = remaining
            else:
                # æŒ‰æƒé‡æ¯”ä¾‹åˆ†é…
                proportion = weights[stage_count] / total_weight
                target_count = max(1, int(total * proportion))
                targets[stage_count] = target_count
                remaining -= target_count
        
        return targets
    
    def _generate_configs_with_fixed_stages(self, num_stages: int, target_count: int, 
                                          seen_configs: set) -> List[Tuple[Dict[str, Any], str]]:
        """ç”Ÿæˆå›ºå®š stage æ•°é‡çš„é…ç½®"""
        configs = []
        attempts = 0
        max_attempts = target_count * 10  # é˜²æ­¢æ— é™å¾ªç¯

        print(f"ğŸ”„ å¼€å§‹ç”Ÿæˆ {target_count} ä¸ª {num_stages} stage çš„é…ç½®...")
        
        while len(configs) < target_count and attempts < max_attempts:
            attempts += 1
            
            config = self.generate_random_config()
            # ç¡®ä¿stageæ•°é‡æ­£ç¡®
            if len(config['stages']) != num_stages:
                continue
            
            config_hash = self._get_config_hash(config)
            if config_hash in seen_configs:
                continue
            
            seen_configs.add(config_hash)
            
            description = self._generate_description(config)
            configs.append((config, description))

            # æ˜¾ç¤ºè¿›åº¦
            if len(configs) % 100 == 0 or len(configs) == target_count:
                print(f"  âœ… å·²ç”Ÿæˆ {len(configs)}/{target_count} ä¸ª {num_stages} stage é…ç½®")
        
        if len(configs) < target_count:
            print(f"âš ï¸  è­¦å‘Š: åªç”Ÿæˆäº† {len(configs)}/{target_count} ä¸ª {num_stages} stage é…ç½®")
        
        return configs
    
    def _get_config_hash(self, config: Dict[str, Any]) -> str:
        """ç”Ÿæˆé…ç½®çš„å”¯ä¸€å“ˆå¸Œå€¼ï¼Œ ç”¨äºå»é‡"""
        hash_parts = []
        
        for i, stage in enumerate(config['stages']):
            stage_hash = f"S{i}_C{stage['channels']}_B{len(stage['blocks'])}"
            
            for j, block in enumerate(stage['blocks']):
                block_hash = f"B{j}_{block['type']}_K{block['kernel_size']}_E{block['expansion']}"
                block_hash += f"_SE{block['has_se']}_{block['se_ratios']}"
                block_hash += f"_Skip{block['skip_connection']}_S{block['stride']}"
                stage_hash += f"_{block_hash}"
            
            hash_parts.append(stage_hash)
        
        return "|".join(hash_parts)
    
    def _generate_description(self, config: Dict[str, Any]) -> str:
        """ç”Ÿæˆé…ç½®çš„æè¿°å­—ç¬¦ä¸²"""
        desc_parts = []
        
        for i, stage in enumerate(config['stages']):
            stage_desc = f"S{i+1}C{stage['channels']}B{len(stage['blocks'])}"
            desc_parts.append(stage_desc)
            
            for j, block in enumerate(stage['blocks']):
                block_desc = f"{block['type']}"
                if block['expansion'] > 1:
                    block_desc += f"Exp{block['expansion']}"
                if block['has_se']:
                    block_desc += f"SE{block['se_ratios']}"
                if block['skip_connection']:
                    block_desc += "Skip"
                if block['stride'] > 1:
                    block_desc += f"S{block['stride']}"
                if j > 0:  # ä¸ºæ‰€æœ‰blockæ·»åŠ ä¿¡æ¯ï¼Œä¸åªæ˜¯ç¬¬ä¸€ä¸ª
                    desc_parts[-1] += f"_{block_desc}"

        # æ·»åŠ éšæœºåç¼€ä»¥ç¡®ä¿å”¯ä¸€æ€§
        import random
        random_suffix = random.randint(1000, 9999)
        return "_".join(desc_parts) + f"_{random_suffix}"


    def create_gpu_processes(self, num_gpus, task_queue, result_queue, dataset_name, base_save_dir, log_dir):
        """åˆ›å»ºGPUå·¥ä½œè¿›ç¨‹"""
        processes = []
        for gpu_id in range(num_gpus):
            p = Process(
                target=gpu_worker,
                args=(gpu_id, task_queue, result_queue, dataset_name, base_save_dir, log_dir)
            )
            p.daemon = True
            p.start()
            processes.append(p)
            time.sleep(1)
        return processes

def check_generated_models(base_save_dir, expected_count):
    """æ£€æŸ¥ç”Ÿæˆçš„æ¨¡å‹æ•°é‡å’Œå®Œæ•´æ€§"""
    print(f"ğŸ” æ£€æŸ¥ç”Ÿæˆçš„æ¨¡å‹åœ¨ç›®å½•: {base_save_dir}")
    
    # è·å–æ‰€æœ‰å­æ–‡ä»¶å¤¹
    subdirectories = [d for d in os.listdir(base_save_dir) if os.path.isdir(os.path.join(base_save_dir, d))]
    
    print(f"æ‰¾åˆ° {len(subdirectories)} ä¸ªå­æ–‡ä»¶å¤¹ (é¢„æœŸ: {expected_count})")
    
    # æ£€æŸ¥æ¯ä¸ªå­æ–‡ä»¶å¤¹çš„æ–‡ä»¶å®Œæ•´æ€§
    incomplete_folders = []
    complete_folders = []
    
    for folder in subdirectories:
        folder_path = os.path.join(base_save_dir, folder)
        files = os.listdir(folder_path)
        
        # expected_files = {"best_model.pth", "model.json", "quant_best_model.pth", "quant_model.json"}
        # æ›´æ–°æœŸæœ›çš„æ–‡ä»¶åˆ—è¡¨ï¼ŒåŒ…å« QAT ç›¸å…³æ–‡ä»¶
        expected_files = {
            "best_model.pth", "model.json", 
            "quant_best_model.pth", "quant_model.json",
            "qat_best_model.pth", "qat_model.json"
        }
        actual_files = set(files)
        
        missing_files = expected_files - actual_files
        extra_files = actual_files - expected_files
        
        if missing_files:
            incomplete_folders.append({
                "folder": folder,
                "missing_files": list(missing_files),
                "extra_files": list(extra_files)
            })
        else:
            complete_folders.append(folder)
    
    # è¾“å‡ºç»“æœ
    print(f"âœ… å®Œæ•´æ–‡ä»¶å¤¹: {len(complete_folders)} ä¸ª")
    print(f"âŒ ä¸å®Œæ•´æ–‡ä»¶å¤¹: {len(incomplete_folders)} ä¸ª")
    
    if incomplete_folders:
        print("\nä¸å®Œæ•´æ–‡ä»¶å¤¹è¯¦æƒ…:")
        for folder_info in incomplete_folders[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  - {folder_info['folder']}: ç¼ºå¤± {folder_info['missing_files']}")
            if folder_info['extra_files']:
                print(f"    é¢å¤–æ–‡ä»¶: {folder_info['extra_files']}")
        
        if len(incomplete_folders) > 10:
            print(f"  ... è¿˜æœ‰ {len(incomplete_folders) - 10} ä¸ªä¸å®Œæ•´æ–‡ä»¶å¤¹æœªæ˜¾ç¤º")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„é…ç½®
    config_hashes = {}
    duplicate_configs = []
    
    for folder in subdirectories:
        model_json_path = os.path.join(base_save_dir, folder, "model.json")
        
        if os.path.exists(model_json_path):
            try:
                with open(model_json_path, 'r') as f:
                    model_data = json.load(f)
                
                config_hash = json.dumps(model_data['config'], sort_keys=True)
                
                if config_hash in config_hashes:
                    duplicate_configs.append({
                        "folder": folder,
                        "duplicate_of": config_hashes[config_hash]
                    })
                else:
                    config_hashes[config_hash] = folder
            except Exception as e:
                print(f"âš ï¸  æ— æ³•è¯»å– {model_json_path}: {e}")
    
    print(f"\nğŸ” é‡å¤é…ç½®æ£€æŸ¥: å‘ç° {len(duplicate_configs)} ä¸ªé‡å¤é…ç½®")
    if duplicate_configs:
        for dup in duplicate_configs[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé‡å¤
            print(f"  - {dup['folder']} é‡å¤äº {dup['duplicate_of']}")
        
        if len(duplicate_configs) > 5:
            print(f"  ... è¿˜æœ‰ {len(duplicate_configs) - 5} ä¸ªé‡å¤é…ç½®æœªæ˜¾ç¤º")
    
    return len(subdirectories), incomplete_folders, duplicate_configs

# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    # ä»å‘½ä»¤è¡Œå‚æ•°ä¸­è·å– dataset_name å’Œå…¶ä»–é…ç½®
    dataset_name = args.dataset_name
    # å®šä¹‰æœç´¢ç©ºé—´
    search_space = {
        "stages": [1, 2, 3, 4],
        "conv_types": ["DWSepConv", "MBConv", "DpConv", "SeSepConv", "SeDpConv"],
        "kernel_sizes": [3, 5, 7],
        "strides": [1, 2, 4],
        "skip_connection": [True, False],
        "activations": ["ReLU6", "LeakyReLU", "Swish"],
        "expansions": [1, 2, 3, 4],
        "channels": [8, 16, 24, 32],
        "has_se": [True, False],
        "se_ratios": [0, 0.25, 0.5],
        "blocks_per_stage": [1, 2],
        "quantization_modes": ["none", "static", "qat"]
    }
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹å¼
    mp.set_start_method('spawn', force=True)
    # è®¾ç½®ä¿¡å·å¤„ç†ï¼Œé¿å…é”®ç›˜ä¸­æ–­æ—¶å‡ºç°åƒµå°¸è¿›ç¨‹
    original_sigint = signal.signal(signal.SIGINT, signal.SIG_IGN)

    try:
        signal.signal(signal.SIGINT, original_sigint)
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        generator = ArchitectureGenerator(search_space, dataset_name=dataset_name, seed=2002)
        
        # ç”Ÿæˆé…ç½®æ•°é‡
        num_configs = 4000  # ç”Ÿæˆ10000ä¸ªä¸åŒçš„æ¶æ„
        num_threads = 4      # ä½¿ç”¨4ä¸ªçº¿ç¨‹

        print(f"å¼€å§‹ä½¿ç”¨ {num_threads} ä¸ªçº¿ç¨‹ç”Ÿæˆ {num_configs} ä¸ªæ¶æ„é…ç½®...")

        # ä½¿ç”¨åˆ†å±‚æŠ½æ ·ç”Ÿæˆé…ç½®
        configs = generator.generate_stratified_configs(num_configs, num_threads)
        
        # ä¿å­˜é…ç½®
        # è®¾ç½®ä¿å­˜ç›®å½•
        base_save_dir = "/root/tinyml/weights/GNNpredictor_data"
        # åˆ›å»ºæ—¶é—´æˆ³å­æ–‡ä»¶å¤¹
        china_timezone = pytz.timezone("Asia/Shanghai")
        timestamp = datetime.now(china_timezone).strftime("%m-%d-%H-%M")
        save_dir = os.path.join(base_save_dir, timestamp)
        os.makedirs(save_dir, exist_ok=True)

        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = os.path.join(save_dir, "logs")
        os.makedirs(log_dir, exist_ok=True)

        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—å’Œç»“æœé˜Ÿåˆ—
        manager = Manager()
        task_queue = manager.Queue()
        result_queue = manager.Queue()

        # å°†é…ç½®æ”¾å…¥ä»»åŠ¡é˜Ÿåˆ—
        for config, description in configs:
            task_queue.put((config, description))

        # åˆ›å»º GPU å·¥ä½œè¿›ç¨‹
        num_gpus = 4
        processes = generator.create_gpu_processes(num_gpus, task_queue, result_queue, 
                                                   generator.dataset_name, save_dir, log_dir)

        # # å‘é€ç»“æŸä¿¡å·ç»™æ‰€æœ‰å·¥ä½œè¿›ç¨‹
        for _ in range(num_gpus):
            task_queue.put(None)

        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
        for p in processes:
            p.join()

        print("âœ… æ‰€æœ‰ GPU å·¥ä½œè¿›ç¨‹å®Œæˆ")

        # æ£€æŸ¥ç”Ÿæˆçš„æ¨¡å‹
        folder_count, incomplete_folders, duplicate_configs = check_generated_models(save_dir, len(configs))

        print(f"ğŸ“ æ¶æ„é…ç½®å·²ä¿å­˜åˆ°: {save_dir}")
        print(f"ğŸ“Š æ—¥å¿—æ–‡ä»¶ä¿å­˜åœ¨: {log_dir}")

    except KeyboardInterrupt:
        print("ğŸ›‘ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()    