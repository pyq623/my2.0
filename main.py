# main.py
import os
import json
import argparse
from datetime import datetime
from mcts import ScatteredForestSearch, create_sfs_search
from configs import get_search_space

def setup_search_space(dataset_name: str = "MMAct") -> dict:
    """è®¾ç½®æœç´¢ç©ºé—´"""
    info = get_search_space()
    search_space = info['search_space']
    constraints = info['constraints']
    return search_space, constraints

def create_output_directory(experiment_name: str) -> str:
    """åˆ›å»ºè¾“å‡ºç›®å½•"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"results/{experiment_name}_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    return output_dir

def save_search_results(search: ScatteredForestSearch, output_dir: str, args: dict):
    """ä¿å­˜æœç´¢ç»“æœ"""
    # ä¿å­˜æœ€ä½³æ¨¡å‹é…ç½®
    best_model = search.get_best_model()
    if best_model:
        best_config_path = os.path.join(output_dir, "best_model_config.json")
        with open(best_config_path, 'w') as f:
            json.dump(best_model.config, f, indent=2, ensure_ascii=False)
        print(f"âœ… æœ€ä½³æ¨¡å‹é…ç½®å·²ä¿å­˜: {best_config_path}")
    
    # ä¿å­˜æœç´¢ç»Ÿè®¡ä¿¡æ¯
    stats = search.get_search_statistics()
    stats_path = os.path.join(output_dir, "search_statistics.json")
    with open(stats_path, 'w') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False, default=str)
    print(f"âœ… æœç´¢ç»Ÿè®¡å·²ä¿å­˜: {stats_path}")
    
    # ä¿å­˜æœç´¢çŠ¶æ€
    state_path = os.path.join(output_dir, "search_state.json")
    search.save_search_state(state_path)
    print(f"âœ… æœç´¢çŠ¶æ€å·²ä¿å­˜: {state_path}")
    
    # ä¿å­˜å®éªŒå‚æ•°
    args_path = os.path.join(output_dir, "experiment_args.json")
    with open(args_path, 'w') as f:
        json.dump(args, f, indent=2, ensure_ascii=False)
    print(f"âœ… å®éªŒå‚æ•°å·²ä¿å­˜: {args_path}")

def print_final_results(search: ScatteredForestSearch):
    """æ‰“å°æœ€ç»ˆç»“æœ"""
    print("\n" + "="*60)
    print("ğŸ‰ æœç´¢å®Œæˆ! æœ€ç»ˆç»“æœ:")
    print("="*60)
    
    best_model = search.get_best_model()
    stats = search.get_search_statistics()
    
    if best_model:
        print(f"ğŸ† æœ€ä½³æ¨¡å‹å¥–åŠ±: {stats['best_reward']:.4f}")
        print(f"ğŸ“Š æ€»è¿­ä»£æ¬¡æ•°: {stats['iterations']}")
        print(f"ğŸŒ³ æœç´¢æ ‘ç»Ÿè®¡:")
        print(f"   - æ€»èŠ‚ç‚¹æ•°: {stats['tree_statistics']['total_nodes']}")
        print(f"   - è¯„ä¼°èŠ‚ç‚¹: {stats['tree_statistics']['evaluated_nodes']}")
        print(f"   - æ£®æ—å¤§å°: {stats['tree_statistics']['forest_count']}")
        print(f"   - å¹³å‡å¥–åŠ±: {stats['tree_statistics']['average_reward']:.4f}")
        
        print("\nğŸ” å…¨å±€ç»éªŒæ´å¯Ÿ:")
        for direction, insight in stats['global_insights'].items():
            if direction.startswith('direction_'):
                success_rate = insight.get('success_rate', 0)
                avg_reward = insight.get('average_reward', 0)
                print(f"   - {direction}: æˆåŠŸç‡={success_rate:.3f}, å¹³å‡å¥–åŠ±={avg_reward:.3f}")
    
    print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•£å°„æ£®æ—æœç´¢ç®—æ³•")
    parser.add_argument("--dataset", type=str, default="MMAct", 
                       choices=["MMAct", "Mhealth", "Wharf", "UTD-MHAD", "USCHAD"], 
                       help="æ•°æ®é›†åç§°")
    parser.add_argument("--iterations", type=int, default=50, 
                       help="æœç´¢è¿­ä»£æ¬¡æ•°")
    parser.add_argument("--num_seeds", type=int, default=2,   # ä»¥åè®¾ç½®ä¸º5
                       help="åˆå§‹ç§å­æ•°é‡")
    parser.add_argument("--device", type=str, default="cuda", 
                       choices=["cuda", "cpu"], 
                       help="è®¡ç®—è®¾å¤‡")
    parser.add_argument("--exploration_weight", type=float, default=1.414, 
                       help="æ¢ç´¢æƒé‡")
    parser.add_argument("--experiment_name", type=str, default="sfs", 
                       help="å®éªŒåç§°")
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹æ•£å°„æ£®æ—æœç´¢...")
    print(f"ğŸ“‹ å®éªŒé…ç½®:")
    print(f"   - æ•°æ®é›†: {args.dataset}")
    print(f"   - è¿­ä»£æ¬¡æ•°: {args.iterations}")
    print(f"   - åˆå§‹ç§å­: {args.num_seeds}")
    print(f"   - è®¾å¤‡: {args.device}")
    print(f"   - æ¢ç´¢æƒé‡: {args.exploration_weight}")
    print(f"   - å®éªŒåç§°: {args.experiment_name}")
    
    # è®¾ç½®æœç´¢ç©ºé—´å’Œçº¦æŸ
    search_space, constraints = setup_search_space(args.dataset)

    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = create_output_directory(args.experiment_name)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    try:
        # åˆ›å»ºæœç´¢å®ä¾‹
        search = create_sfs_search(
            search_space=search_space,
            constraints=constraints,
            device=args.device,
            exploration_weight=args.exploration_weight,
            dataset_name=args.dataset
        )
        
        # åˆå§‹åŒ–æ£®æ—
        print("\nğŸŒ± åˆå§‹åŒ–æ£®æ—...")
        search.initialize_forest(num_seeds=args.num_seeds)
        
        # æ‰§è¡Œæœç´¢
        print(f"\nğŸ” å¼€å§‹æœç´¢ ({args.iterations} æ¬¡è¿­ä»£)...")
        search.search(
            iterations=args.iterations,
            exploration_weight=args.exploration_weight,
            dataset_names=[args.dataset]
        )
        
        # ä¿å­˜ç»“æœ
        print("\nğŸ’¾ ä¿å­˜æœç´¢ç»“æœ...")
        save_search_results(search, output_dir, vars(args))
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        print_final_results(search)
        
        print(f"\nâœ… å®éªŒå®Œæˆ! ç»“æœä¿å­˜åœ¨: {output_dir}")
        
    except Exception as e:
        print(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()