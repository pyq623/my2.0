from typing import List, Dict, Any, Optional, Tuple
from data import get_dataset_info, get_multitask_dataloaders
from queue import Empty
import time
import torch.multiprocessing as mp
from models import CandidateModel
from models import QuantizableModel
from utils import calculate_memory_usage
import copy
import torch
from models import get_quantization_option, apply_configurable_static_quantization
from models import fuse_QATmodel_modules, prepare_qaft_model, apply_qaft_quantization  # âœ… æ–°å¢

class ParallelModelEvaluator:
    """
    å¹¶è¡Œæ¨¡å‹è¯„ä¼°å™¨ - å·¥ä½œè¿›ç¨‹ä½¿ç”¨
    æ¯ä¸ªå·¥ä½œè¿›ç¨‹æœ‰ä¸€ä¸ªç‹¬ç«‹çš„è¯„ä¼°å™¨å®ä¾‹
    """
    def __init__(self, gpu_id: int, constraints: Dict[str, float], 
                 dataset_name: str, train_epochs: int = 100):
        self.gpu_id = gpu_id
        self.constraints = constraints
        self.dataset_name = dataset_name
        self.train_epochs = train_epochs
        
        # è®¾ç½®è®¾å¤‡
        self.device = f"cuda:{gpu_id}"
        
        # åŠ è½½æ•°æ®é›†
        self.dataset_info = get_dataset_info(dataset_name)
        multitask_dataloaders = get_multitask_dataloaders(
            root_dir="/root/har_train/data/UniMTS_data", 
            datasets=[dataset_name]
        )
        self.dataloader = multitask_dataloaders[dataset_name]
        
        print(f"[GPU {gpu_id}] è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def evaluate(self, config: Dict[str, Any], candidate_id: str) -> Tuple[float, Dict[str, Any]]:
        """è¯„ä¼°å•ä¸ªé…ç½®"""
        try:
            print(f"[GPU {self.gpu_id}] å¼€å§‹è¯„ä¼° {candidate_id}")
            
            # åˆ›å»ºå€™é€‰æ¨¡å‹
            candidate = CandidateModel(config=config)
            candidate.candidate_id = candidate_id
            
            # 2. è·å–é‡åŒ–æ¨¡å¼
            quant_mode = config.get('quant_mode', 'none')
            self._current_quant_mode = quant_mode  # âœ… ä¿å­˜å½“å‰é‡åŒ–æ¨¡å¼
            print(f"[GPU {self.gpu_id}] é‡åŒ–æ¨¡å¼: {quant_mode}")

            # 3. æ„å»ºæ¨¡å‹
            model = candidate.build_model()
            
            # ========== å…¬å¹³çš„è®­ç»ƒç­–ç•¥ ==========
            # QAFT å‡†å¤‡
            if quant_mode == 'qaft':
                print(f"[GPU {self.gpu_id}] âš–ï¸ QAFT æ¨¡å¼ - ä¸¤é˜¶æ®µè®­ç»ƒ (æ€»è®¡{self.train_epochs}è½®)")
                
                # è®¡ç®—è½®æ•°åˆ†é… (90% + 10%)
                pretrain_epochs = int(self.train_epochs * 0.9)  # 90è½®
                finetune_epochs = self.train_epochs - pretrain_epochs  # 10è½®
                
                print(f"[GPU {self.gpu_id}] ğŸ“Š è®­ç»ƒåˆ†é…:")
                print(f"  - é˜¶æ®µ1(é¢„è®­ç»ƒ): {pretrain_epochs}è½®")
                print(f"  - é˜¶æ®µ2(å¾®è°ƒ): {finetune_epochs}è½®")
                print(f"  - æ€»è®¡: {pretrain_epochs + finetune_epochs}è½®")
                
                # âœ… é˜¶æ®µ1: æ­£å¸¸è®­ç»ƒé¢„çƒ­
                print(f"[GPU {self.gpu_id}] ğŸ”¥ é˜¶æ®µ1: é¢„è®­ç»ƒå¼€å§‹...")
                from training import SingleTaskTrainer
                
                pretrain_trainer = SingleTaskTrainer(
                    model=model,
                    dataloaders=self.dataloader,
                    device=self.device
                )
                
                pretrain_accuracy, pretrain_metrics, _, pretrain_state = pretrain_trainer.train(
                    epochs=pretrain_epochs,
                    save_path=f"checkpoints/pretrain_{candidate_id}.pth"
                )
                
                print(f"[GPU {self.gpu_id}] âœ… é˜¶æ®µ1å®Œæˆ: å‡†ç¡®ç‡={pretrain_accuracy:.2f}%")
                
                # âœ… é˜¶æ®µ2: QAFT å¾®è°ƒ
                print(f"[GPU {self.gpu_id}] ğŸ¯ é˜¶æ®µ2: QAFTå¾®è°ƒå¼€å§‹...")
                
                # åŠ è½½æœ€ä½³é¢„è®­ç»ƒæƒé‡
                if pretrain_state and 'model' in pretrain_state:
                    model.load_state_dict(pretrain_state['model'])
                    print(f"[GPU {self.gpu_id}] ğŸ“¥ å·²åŠ è½½é¢„è®­ç»ƒæƒé‡")
                
                # å‡†å¤‡QAFT (æ’å…¥FakeQuantize)
                model = self._prepare_qat_model(model)
                
                # å¾®è°ƒè®­ç»ƒ
                qaft_trainer = SingleTaskTrainer(
                    model=model,
                    dataloaders=self.dataloader,
                    device=self.device
                )
                
                best_accuracy, best_val_metrics, history, best_model_state = qaft_trainer.train(
                    epochs=finetune_epochs,
                    save_path=f"checkpoints/candidate_{candidate_id}.pth"
                )
                
                print(f"[GPU {self.gpu_id}] âœ… é˜¶æ®µ2å®Œæˆ: å‡†ç¡®ç‡={best_accuracy:.2f}%")
                print(f"[GPU {self.gpu_id}] ğŸ“ˆ å‡†ç¡®ç‡å˜åŒ–: {pretrain_accuracy:.2f}% â†’ {best_accuracy:.2f}%")

            elif quant_mode == 'qat':
                # âœ… QAT: æ’å…¥é‡åŒ–åå®Œæ•´è®­ç»ƒ
                print(f"[GPU {self.gpu_id}] âš¡ QATæ¨¡å¼ - é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ ({self.train_epochs}è½®)")
                model = self._prepare_qat_model(model)
                
                from training import SingleTaskTrainer
                trainer = SingleTaskTrainer(
                    model=model,
                    dataloaders=self.dataloader,
                    device=self.device
                )
                
                best_accuracy, best_val_metrics, history, best_model_state = trainer.train(
                    epochs=self.train_epochs,
                    save_path=f"checkpoints/candidate_{candidate_id}.pth"
                )
                
                print(f"[GPU {self.gpu_id}] âœ… QAT è®­ç»ƒå®Œæˆ: å‡†ç¡®ç‡={best_accuracy:.2f}%")

            else:
                # âœ… None/Static: æ­£å¸¸è®­ç»ƒ
                print(f"[GPU {self.gpu_id}] ğŸ”„ {quant_mode.upper()}æ¨¡å¼ - æ­£å¸¸è®­ç»ƒ ({self.train_epochs}è½®)")
                from training import SingleTaskTrainer
                trainer = SingleTaskTrainer(
                    model=model,
                    dataloaders=self.dataloader,
                    device=self.device
                )
                
                best_accuracy, best_val_metrics, history, best_model_state = trainer.train(
                    epochs=self.train_epochs,
                    save_path=f"checkpoints/candidate_{candidate_id}.pth"
                )
                
                print(f"[GPU {self.gpu_id}] âœ… è®­ç»ƒå®Œæˆ: å‡†ç¡®ç‡={best_accuracy:.2f}%")
            
            # ========== åç»­ä»£ç ä¸å˜ ==========
            # 6. æµ‹é‡åŸå§‹æ¨¡å‹æ€§èƒ½
            memory_usage = calculate_memory_usage(
                model,
                input_size=(64, self.dataset_info['channels'], self.dataset_info['time_steps']),
                device='cpu'
            )
            print(f"[GPU {self.gpu_id}] åŸå§‹å†…å­˜: {memory_usage['total_memory_MB']:.2f}MB")
            # å¥–åŠ±å°±æ˜¯å‡†ç¡®ç‡
            reward = best_accuracy
            accuracy = best_accuracy
            
            metrics = {
                "original_accuracy": accuracy,
                "original_accuracy_percent": best_accuracy,
                "original_memory": memory_usage['total_memory_MB'],
                "reward": reward,
                "train_loss": best_val_metrics.get('loss', 0.0) if best_val_metrics else 0.0,
                # "training_history": history,
                "gpu_id": self.gpu_id,
                'quantization_mode': quant_mode
            }
            
            # 8. é‡åŒ–å¤„ç† (å¦‚æœéœ€è¦)
            if quant_mode != 'none':
                quant_accuracy, quant_metrics = self._apply_quantization(
                    model, best_model_state, quant_mode, candidate_id
                )

                accuracy_drop = best_accuracy - quant_accuracy

                # æ›´æ–°æŒ‡æ ‡
                metrics.update({
                    'quantized_accuracy': quant_metrics.get('accuracy', 0),
                    'quantized_memory': quant_metrics.get('peak_memory', memory_usage['total_memory_MB']),
                    'quantization_method': quant_metrics.get('method', 'unknown'),
                    'accuracy_drop': accuracy_drop,
                    'quantization_save_path': quant_metrics.get('save_path', None)
                })
                
                # é‡åŒ–åè‡ªç„¶æ˜¯é€‰æ‹©é‡åŒ–åçš„å‡†ç¡®ç‡
                reward = quant_accuracy
                metrics['accuracy'] = quant_accuracy
                print(f"[GPU {self.gpu_id}] ä½¿ç”¨é‡åŒ–å¥–åŠ±: {reward:.2f}")
                print(f"  - åŸå§‹å‡†ç¡®ç‡: {best_accuracy:.2f}%")
                print(f"  - é‡åŒ–å‡†ç¡®ç‡: {quant_accuracy:.2f}%")
                print(f"  - å‡†ç¡®ç‡ä¸‹é™: {accuracy_drop:.2f}%")
                print(f"  - ä½¿ç”¨é‡åŒ–å¥–åŠ±: {reward:.2f}")
            else:
                reward = best_accuracy
                metrics['accuracy'] = best_accuracy
                print(f"[GPU {self.gpu_id}] æœªè¿›è¡Œé‡åŒ–ï¼Œä½¿ç”¨åŸå§‹å¥–åŠ±: {reward:.2f}")
            
            metrics['reward'] = reward
            print(f"[GPU {self.gpu_id}] å®Œæˆè¯„ä¼° {candidate_id}: å¥–åŠ±={reward:.2f}")
            
            return reward, metrics
            
        except Exception as e:
            print(f"[GPU {self.gpu_id}] è¯„ä¼°å¤±è´¥ {candidate_id}: {e}")
            import traceback
            traceback.print_exc()
            return 0.0, {
                "error": str(e),
                "accuracy": 0.0,
                "reward": 0.0,
                "gpu_id": self.gpu_id
            }
        
    def _prepare_qat_model(self, model):
        """å‡†å¤‡QATé‡åŒ–æ„ŸçŸ¥è®­ç»ƒæ¨¡å‹"""
        try:

            # è·å–é‡åŒ–æ¨¡å¼
            quant_mode = getattr(self, '_current_quant_mode', 'qat')
            if quant_mode == 'qaft':
                print(f"[GPU {self.gpu_id}] å‡†å¤‡QAFTé‡åŒ– (å¾®è°ƒæ¨¡å¼)")
                model = prepare_qaft_model(model, freeze_backbone=True)
            else:
                print(f"[GPU {self.gpu_id}] å‡†å¤‡QATé‡åŒ– (å®Œæ•´è®­ç»ƒ)")
                model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
                fuse_QATmodel_modules(model)
                model.train()
                torch.quantization.prepare_qat(model, inplace=True)
            print(f"[GPU {self.gpu_id}] é‡åŒ–å‡†å¤‡å®Œæˆ")
            return model
            
        except Exception as e:
            print(f"[GPU {self.gpu_id}] é‡åŒ–å‡†å¤‡å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return model
    
    def _apply_quantization(self, model, best_state: dict, quant_mode: str, 
                           candidate_id: str) -> Tuple[float, Dict[str, Any]]:
        """åº”ç”¨é‡åŒ–å¹¶è¯„ä¼°"""
        try:
            print(f"[GPU {self.gpu_id}] åº”ç”¨{quant_mode}é‡åŒ–")
            
            # æ ¹æ®é‡åŒ–æ¨¡å¼é€‰æ‹©é…ç½®
            if quant_mode == 'static':
                quantization_options = [
                    ('int8_default', 'é»˜è®¤INT8é‡åŒ–'),
                    ('int8_per_channel', 'é€é€šé“INT8é‡åŒ–'),
                    ('int8_reduce_range', 'å‡å°‘èŒƒå›´INT8é‡åŒ–'),
                    ('int8_asymmetric', 'INT8éå¯¹ç§°é‡åŒ–'),
                    ('int8_histogram', 'INT8ç›´æ–¹å›¾æ ¡å‡†'),
                    ('int8_moving_avg', 'INT8ç§»åŠ¨å¹³å‡æ ¡å‡†')
                ]
            elif quant_mode == 'qat':
                quantization_options = [('qat_default', 'QATé‡åŒ–')]
            elif quant_mode == 'qaft':
                # âœ… QAFTä½¿ç”¨ç‰¹æ®Šå¤„ç†
                print(f"[GPU {self.gpu_id}] QAFTæ¨¡å¼ - ç›´æ¥è½¬æ¢")
                quantization_options = [('qaft_default', 'QAFTé‡åŒ–')]
            else:
                return 0.0, {}
            
            best_accuracy = 0.0
            best_quantized_model = None
            best_memory = 0.0
            best_option_name = ""
            
            # å°è¯•ä¸åŒé‡åŒ–é…ç½®
            for option_name, option_desc in quantization_options:
                try:
                    print(f"[GPU {self.gpu_id}] å°è¯• {option_desc}")
                    
                    quantized_model = self._apply_quantization_helper(
                        model, quant_mode, option_name
                    )
                    
                    if quantized_model:
                        # åˆ›å»ºä»»åŠ¡å¤´å¹¶åŠ è½½æƒé‡
                        import torch.nn as nn
                        task_head = nn.Linear(
                            model.output_dim,
                            len(self.dataloader['test'].dataset.classes)
                        ).to('cpu')
                        
                        if best_state and 'head' in best_state:
                            task_head.load_state_dict(best_state['head'])
                        
                        # è¯„ä¼°é‡åŒ–æ¨¡å‹
                        from models import evaluate_quantized_model
                        quant_accuracy = evaluate_quantized_model(
                            quantized_model, self.dataloader, task_head,
                            f"é‡åŒ–æ¨¡å‹({option_name})"
                        )

                         # æµ‹é‡å†…å­˜ (åªæµ‹é‡ä¸€æ¬¡)
                        quant_memory = calculate_memory_usage(
                            quantized_model,
                            input_size=(64, self.dataset_info['channels'], self.dataset_info['time_steps']),
                            device='cpu'
                        )['total_memory_MB']
                        
                        print(f"[GPU {self.gpu_id}] ğŸ“Š {option_desc}: "
                              f"{quant_accuracy:.1f}% / {quant_memory:.2f}MB")
                        
                        # è®°å½•æœ€ä½³ç»“æœ
                        if quant_accuracy > best_accuracy:
                            best_accuracy = quant_accuracy
                            best_quantized_model = quantized_model
                            best_memory = quant_memory
                            best_option_name = option_name
                
                except Exception as e:
                    print(f"[GPU {self.gpu_id}] {option_desc} å¤±è´¥: {e}")
                    continue
            
            # ä¿å­˜æœ€ä½³é‡åŒ–æ¨¡å‹
            if best_quantized_model:
                import torch
                quant_save_path = f"checkpoints/quant_{candidate_id}_{best_option_name}.pth"
                torch.save(best_quantized_model.state_dict(), quant_save_path)
                
                print(f"[GPU {self.gpu_id}] æœ€ä½³é‡åŒ–: {best_option_name}, "
                      f"å‡†ç¡®ç‡={best_accuracy:.1f}%")
                
                return best_accuracy, {
                    'accuracy': best_accuracy,
                    'method': best_option_name,
                    'peak_memory': best_memory,
                    'save_path': quant_save_path
                }
            
            return 0.0, {}
            
        except Exception as e:
            print(f"[GPU {self.gpu_id}] é‡åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0.0, {}
    
    def _apply_quantization_helper(self, model, quant_mode: str, 
                                   quantization_option: str = 'int8_per_channel'):
        """é‡åŒ–è¾…åŠ©æ–¹æ³•"""
        
        model_copy = copy.deepcopy(model)
        model_copy.to('cpu').eval()
        
        if quant_mode == 'static':
            # é™æ€é‡åŒ–
            quant_config = get_quantization_option(quantization_option)
            print(f"[GPU {self.gpu_id}] é‡åŒ–é…ç½®: {quant_config['description']}")
            
            quantized_model = apply_configurable_static_quantization(
                model_copy,
                self.dataloader,
                precision=quant_config['precision'],
                backend=quant_config['backend']
            )
            
        elif quant_mode in ['qat', 'qaft']:
            # âœ… QATå’ŒQAFTéƒ½ä½¿ç”¨convertè½¬æ¢
            print(f"[GPU {self.gpu_id}] è½¬æ¢{quant_mode.upper()}æ¨¡å‹")
            model_copy.eval()
            quantized_model = torch.quantization.convert(model_copy, inplace=False)
        else:
            print(f"[GPU {self.gpu_id}] æœªçŸ¥é‡åŒ–æ¨¡å¼: {quant_mode}")
            return model_copy
            
        return quantized_model


def worker_process(gpu_id: int, task_queue: mp.Queue, result_queue: mp.Queue,
                   constraints: Dict[str, float], dataset_name: str, 
                   train_epochs: int):
    """
    å·¥ä½œè¿›ç¨‹å‡½æ•°
    
    å‚æ•°:
        gpu_id: GPU ID
        task_queue: ä»»åŠ¡é˜Ÿåˆ—ï¼ˆæ¥æ”¶é…ç½®ï¼‰
        result_queue: ç»“æœé˜Ÿåˆ—ï¼ˆè¿”å›è¯„ä¼°ç»“æœï¼‰
        constraints: çº¦æŸæ¡ä»¶
        dataset_name: æ•°æ®é›†åç§°
        train_epochs: è®­ç»ƒè½®æ•°
    """
    print(f"[Worker-GPU{gpu_id}] å¯åŠ¨")
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ParallelModelEvaluator(
        gpu_id=gpu_id,
        constraints=constraints,
        dataset_name=dataset_name,
        train_epochs=train_epochs
    )
    
    while True:
        try:
            # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ï¼ˆè¶…æ—¶60ç§’ï¼‰
            task = task_queue.get(timeout=60)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç»ˆæ­¢ä¿¡å·
            if task is None:
                print(f"[Worker-GPU{gpu_id}] æ”¶åˆ°ç»ˆæ­¢ä¿¡å·")
                break
            
            # è§£åŒ…ä»»åŠ¡
            candidate_id, config = task
            
            # è¯„ä¼°
            reward, metrics = evaluator.evaluate(config, candidate_id)
            
            # è¿”å›ç»“æœ
            result_queue.put((candidate_id, reward, metrics))
            
        except Empty:
            # é˜Ÿåˆ—è¶…æ—¶ï¼Œç»§ç»­ç­‰å¾…
            continue
        except Exception as e:
            print(f"[Worker-GPU{gpu_id}] å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"[Worker-GPU{gpu_id}] é€€å‡º")